---
title: "Stats 102B HW 4"
author: "Bryan Mui - UID 506021334"
format:
  pdf:
    geometry: left=0.3in, right=0.3in, top=0.3in, bottom=0.3in
    keep-tex: true
    include-in-header:
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
         \DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\{\}}
---

Due Wed, June 4, 11:00 pm

```{r read-data}
library(tidyverse)
train <- read_csv("train_data.csv")
val <- read_csv("validation_data.csv")
```

\newpage

## Problem 1

Consider the function

$$
f(x) = \frac{1}{4} x^4 - x^2 + 2x
$$

### Part $(\alpha)$

Using the pure version of Newton’s algorithm report $x_k$ for $k = 20$ (after running the algorithm for 20 iterations) based on the following 5 initial points:

1.  $x_0 = −1$
2.  $x_0 = 0$
3.  $x_0 = 0.1$
4.  $x_0 = 1$
5.  $x_0 = 2$

**Newton's pure algorithm is as follows:**

1.  Select $x_0 \in \mathbb{R}^n$
2.  While stopping criterion \> tolerance do:
    1.  $x_{k+1} = x_k - [\nabla^2f(x_k)]^{-1} \nabla f(x_k)$
    2.  Calculate value of stopping criterion($|f(x_{k+1}) - f(x_k)| \leq \epsilon$)

Gradient: $\nabla f(x) = \frac{\partial}{\partial x} = f'(x) = x^3 - 2x + 2$

Hessian: $\nabla^2 f(x) = \frac{\partial^2}{\partial x^2} = f''(x) = 3x^2 - 2$

```{r, 1-alpha}
# params
max_iter <- 20
starting_points <- c(-1, 0, 0.1, 1, 2)
stopping_tol <- 1e-6


# algorithm
newton_pure_alg <- function(max_iter, starting_point, stopping_tol) {
  beta <- starting_point
  iterations_ran <- 0
  betas_vec <- c(beta)
  
  obj <- function(x) {
  return(1/4 * x^4 - x^2 + 2*x)
  }
  grad <- function(x) {
    x^3 - 2*x + 2
  }
  hessian <- function(x) {
    3*x^2 - 2
  }
  
  for(i in 1:max_iter) {
    beta_new <- beta - (grad(beta) / hessian(beta))
    betas_vec[i+1] <- beta_new
    if(abs(beta_new - beta) <= stopping_tol) { break }
    beta <- beta_new
  }
  iterations_ran <- i
  return(list(iterations=iterations_ran, betas=betas_vec))
}

# running the alg
cat("Newton's Algorithm(Pure) For Different Starting Points: \n")
for (starting_point in starting_points) {
  result <- newton_pure_alg(max_iter, starting_point, stopping_tol)
  cat("\nStarting Point:", starting_point, "\nIterations:", result$iterations, "\nBetas:\n")
  print(result$betas)
  cat("\n", "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~", "\n")
}
```

### Part (i) What do you observe?

![Plot of the Objective Function f(x)](images/clipboard-3935267595.png)

I observe that given the code output, the algorithms that did not converge oscillate between 0 1 0 1, which means that it gets stuck when the curvature of the graph changes. I think that the graph goes from concave up to concave down at this point, which will mess up the gradient descent calculation. The Hessian is the second derivative, or second order gradient, meaning the concavity of the function will affect the sign. Given the update step, $x_{k+1} = x_k - [\nabla^2f(x_k)]^{-1} \nabla f(x_k)$, we can see that if the Hessian's sign goes from negative to positive(concave down to concave up) or vice versa then it will change the sign of the update step to be gradient ascent(away from the minimum) rather than towards it

### Part (ii) How can you fix the issue reported in (i)?

I believe that the issue is normally fixed by drawing the graph and getting and understanding of the shape, and therefore choosing one of the starting points that does end up converging. However, in cases where that isn't possible(if the function cannot be plotted or is very complex), I would try to build a vector of possible starting points and perform a grid search where we run the algorithm on multiple starting points and various parameters. This would allow us to find a starting point where the algorithm would converge. From the lecture slides, there are alternative Newton's algorithms like Newton's algorithm with damping, which I would implement if there is too much trouble with the pure Newton's algorithm.

\newpage

## Problem 2

Consider the data in the train data.csv file. The first 600 columns correspond to the predictors and the last column to the response y.

### Part (i) Implement that proximal gradient algorithm for Lasso regression, by modifying appropriately your code from Homework 1.

To select a good value for the regularization parameter $λ$ use the data in the validation data.csv to calculate the sum-of-squares error validation loss.

Proximal Gradient Descent is good for problems in the form:

$$
\underset{x}{\min} F(x) = f(x) + g(x), \space x \in \mathbb{R}^n
$$

Where f and g are functions with a global minimum, f is differentiable, and g is not differentiable. In this case, we have $MSE=f(x)=\frac{1}{n}||y-X\beta||_2^2$ and $g(x)=\lambda||\beta||_1$

**The Proximal Gradient Descent Algorithm(General):**

1.  Select $x_0 \in \mathbb{R}^n$
2.  While stopping criterion \> tolerance do:
    1.  $y_k = x_k + \eta_k \nabla{f(x_k)}$
    2.  $x_{k+1} = \text{prox}_{\eta_k,g}(y_k)$
    3.  Calculate value of stopping criterion($|f(x_{k+1}) - f(x_k)| \leq \epsilon$)

Proximal Operator: $\text{prox}_{\eta_k,g}(y_k) = \underset{x}{\text{argmin}}\{ g(x) + \frac{1}{2\eta_k}||x-y_k||^2_2 \}$, where,

-   $\eta_k$ is the step size at the previous iteration,

-   $y_k$ is the gradient step at the previous iteration,

-   $\text{argmin}$ is the minimum value of the function

The Proximal Function for L1-Regularization(LASSO), as shown in class:

$$
\text{prox}_{t,\lambda||\cdot||_1}(z) = S_{t,\lambda}(z) = \text{sign}(z) \cdot \max(|z|-t\lambda,\space0)
$$

```{r 2-1}
# Params
X_train <- train %>%
  select(-y) %>%
  as.matrix()
y_train <- train %>%
  select(y) %>%
  as.matrix()
X_val <- val %>%
  select(-y) %>%
  as.matrix()
y_val <- val %>%
  select(y) %>%
  as.matrix()

etas <- c(1e-3, 5e-3, 1e-2, 5e-2, 1e-1)
lambdas <- c(0.001, 0.01, 0.1, 1, 10)
tol <- 1e-6
max_iter <- 10000

# Implement Proximal Gradient Descent
proximal_gradient_descent <- function(X, y, eta, lambda, tol, max_iter) {
  n <- nrow(X)
  p <- ncol(X)
  beta <- rep(0, p)
  obj_values <- c() # obj values result
  eta_values <- c()  # eta values result
  beta_values <- matrix(0, nrow = max_iter, ncol = p)
  
  # Objective function: Mean Squared Error (MSE)
  obj_function <- function(beta) {
    sum((X %*% beta - y)^2) / (2 * n)
  }
  
  # Gradient function
  gradient <- function(beta) {
    t(X) %*% (X %*% beta - y) / n
  }
  
  # Proximal Function
  prox <- function(eta, lambda, y_k) {
    sign(y_k) * pmax(abs(y_k) - eta * lambda, 0)
  }
  
  for (iter in 1:max_iter) {
    grad <- gradient(beta)
    
    y_k <- beta - eta * grad
    beta_new <- prox(eta, lambda, y_k)
    
    eta_values[iter] <- eta
    beta_values[iter, ] <- beta_new
    obj_values[iter] <- obj_function(beta_new)
    
    if (norm(beta_new - beta, type = "2") < tol) {break}
    
    beta <- beta_new
  }
  
  return(list(iterations=iter, beta_fin=beta, beta_values=beta_values, obj_values=obj_values, eta_values=eta_values, eta=eta, lambda=lambda))
}

# Running the Code
cat("Proxmial Gradient Descent With Lasso: \n")

for (eta in etas) {
  for (lambda in lambdas) {
      result <- proximal_gradient_descent(X_train, y_train, eta, lambda, tol, max_iter)
    cat("\nModel:", 1, "Eta:", result$eta, "Lambda:", result$lambda, "Iterations:", result$iterations, "\n")
    cat("Last 10 Obj-func values: \n")
    tail(result$obj_values, 10)
    cat("\n", "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~", "\n")
  }
}
```

### Part(ii) Show a plot of the training and validation loss as a function of iterations. Report the number of regression coefficients estimated as zero based on the best value of $λ$ you selected.

Getting the Best Candidates:

Plotting Training Loss:

```{r 2-2-train-loss}

```

Plotting Validation Loss:

```{r 2-2-validation-loss}

```
