import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, auc, roc_auc_score
import itertools

# Parameters
seed_parameter = 123
n = 10000
p = 100
corr_factor = 0.5
flip_factor = 0.15
epochs = 50
batch_sizes = [16, 64, 256]
hidden_layers = [[32, 128, 512], [64, 128]]  # two-layer configurations
base_lr = 0.1
decay_rate = 0.7

# Set seeds for reproducibility
np.random.seed(seed_parameter)
torch.manual_seed(seed_parameter)

# Data generation
def generate_correlated_data(n_samples=n, n_features=p, rho=corr_factor, flip_f=flip_factor, seed=None):
    if seed is not None:
        np.random.seed(seed)
    
    # Create correlation matrix
    Sigma = np.ones((n_features, n_features)) * rho
    np.fill_diagonal(Sigma, 1)
    
    # Generate multivariate normal data
    X = np.random.multivariate_normal(mean=np.zeros(n_features), cov=Sigma, size=n_samples)
    
    # Generate coefficients
    beta = np.random.uniform(-1, 1, n_features)
    
    # Calculate logits and probabilities
    logits = X @ beta
    probs = 1 / (1 + np.exp(-logits))
    
    # Generate binary outcomes
    y = np.random.binomial(1, probs)
    
    # Flip some labels to add noise
    flip_idx = np.random.choice(n_samples, size=int(flip_f * n_samples), replace=False)
    y[flip_idx] = 1 - y[flip_idx]
    
    # Combine into dataframe
    data = pd.DataFrame(X, columns=[f'X{i+1}' for i in range(n_features)])
    data.insert(0, 'y', y)
    
    return data

# Generate data
data = generate_correlated_data(n, p, corr_factor, flip_factor, seed_parameter)
X = data.iloc[:, 1:].values
Y = data.iloc[:, 0].values

# Train/Val/Test split
train_idx = np.arange(0, int(0.6 * n))
val_idx = np.arange(int(0.6 * n), int(0.8 * n))
test_idx = np.arange(int(0.8 * n), n)

X_train, Y_train = X[train_idx], Y[train_idx]
X_val, Y_val = X[val_idx], Y[val_idx]
X_test, Y_test = X[test_idx], Y[test_idx]

# Convert to PyTorch tensors
x_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(Y_train.reshape(-1, 1), dtype=torch.float32)
x_val = torch.tensor(X_val, dtype=torch.float32)
y_val = torch.tensor(Y_val.reshape(-1, 1), dtype=torch.float32)
x_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(Y_test.reshape(-1, 1), dtype=torch.float32)

# MLP model definition
class MLPModel(nn.Module):
    def __init__(self, input_dim, hidden1, hidden2):
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden1)
        self.fc2 = nn.Linear(hidden1, hidden2)
        self.fc3 = nn.Linear(hidden2, 1)
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.sigmoid(self.fc3(x))
        return x

# Generate all combinations of hidden layer sizes
hidden_combinations = list(itertools.product(hidden_layers[0], hidden_layers[1]))

# Training loop
results = {}
best_val_loss = float('inf')
best_model = None
best_config = None

for batch_size in batch_sizes:
    for h1, h2 in hidden_combinations:
        # Print the model configuration being used
        print(f"Trying model with h1 = {h1} and h2 = {h2}")
        
        # Initialize model and optimizer
        model = MLPModel(p, h1, h2)
        optimizer = optim.SGD(model.parameters(), lr=base_lr)
        loss_fn = nn.BCELoss()
        
        train_loss_history = []
        val_loss_history = []
        
        # Create data loaders
        train_dataset = TensorDataset(x_train, y_train)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        for epoch in range(epochs):
            # Learning rate decay
            lr = base_lr * (decay_rate ** epoch)
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr
            
            # Training loop
            model.train()
            for x_batch, y_batch in train_loader:
                optimizer.zero_grad()
                y_pred = model(x_batch)
                loss = loss_fn(y_pred, y_batch)
                loss.backward()
                optimizer.step()
            
            # Track training and validation loss
            model.eval()
            with torch.no_grad():
                train_pred = model(x_train)
                val_pred = model(x_val)
                train_loss = loss_fn(train_pred, y_train).item()
                val_loss = loss_fn(val_pred, y_val).item()
            
            train_loss_history.append(train_loss)
            val_loss_history.append(val_loss)
        
        # Store the results
        key = f"Batch_{batch_size}_Hidden_{h1}_{h2}"
        results[key] = {'train': train_loss_history, 'val': val_loss_history}
        
        # Update best model if needed
        if min(val_loss_history) < best_val_loss:
            best_val_loss = min(val_loss_history)
            best_model = model
            best_config = key

print(f"Best model config: {best_config}")

# Plotting training and validation loss
plt.figure(figsize=(12, 10))

# Create figure for training loss
plt.subplot(2, 1, 1)
for key in results:
    plt.plot(range(1, epochs + 1), results[key]['train'], label=key)
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

# Create figure for validation loss
plt.subplot(2, 1, 2)
for key in results:
    plt.plot(range(1, epochs + 1), results[key]['val'], linestyle='dashed', label=key)
plt.title('Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('loss_curves.png')
plt.close()

