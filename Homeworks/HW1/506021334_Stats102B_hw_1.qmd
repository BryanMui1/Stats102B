---
title: "HW 1"
author: "Bryan Mui - UID 506021334 - 14 April 2025"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
         \DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\{\}}
---

```{r options}
#| include: false
options(max.print = 50)
library(ggplot2)
```


# Problem 1 

## Part a 

Find the theoretical min for the function: 
$$
f(x) = x^4 + 2x^2 + 1
$$

Solution: find f'(x) and f''(x), set f'(x) to 0 and solve, and f''(x) needs to be > 0 to be a min

Step 1: find f'(x) and f''(x)
\begin{align}
  f(x) &= x^4 + 2x^2 + 1 \\ 
 f'(x) &= 4x^3 + 4x  \\
f''(x) &= 12x^2 + 4 \\
\end {align}

Step 2: set f'(x) to 0 and solve

\begin{align}
f'(x) &= 4x^3 + 4x  \\
    0 &= 4x^3 + 4x \\
    0 &= 4x(x^2 + 4)
\end{align}


We get $$x = 0$$ and $$0 = x^2 +4$$ which has no real solution

Step 3: check that f''(x) needs to be > 0 to be a min

Our critical point is x = 0,  

\begin{align}
f''(0)  &= 12(0)^2 + 4 \\
        &= 4
\end{align}

Since f'(x) = 0 at 0 and f''(x) > 0 at that point, **we have a min at x = 0, and plugging into f(0) we get the minimum point**
\begin{center} (0, 1) \end{center}  


## Part b 

### 0) 

Use the gradient descent algorithm with **constant step size** and with **back-tracking line search** to calculate $x_{min}$

**Constant step size descent is implemented as follows:**

1. Select a random starting point $x_0$    
2. While stopping criteria < tolerance, do:  

  * Select $η_k$(as a constant)  
  * Calculate $x_{(k+1)} = x_k - η_k * ∇(f(x_k))$  
  * Calculate the value of stopping criterion  
  
Stopping criteria: Stop if $∣|∇(f(x_k)||_2 ≤ ϵ$ 
  

```{r grad_descent_constant_step}
# Gradient descent algorithm that uses backtracking to minimize an objective function

gradient_descent_constant_step <- function(tol = 1e-6, max_iter = 10000, step_size = 0.01) {
  # Step 1: Initialize and select a random stopping point
  # Initialize
  set.seed(777) # example seeding 
  last_iter <- 0 # the last iteration ran
  eta <- step_size # step size that is decided manually 
  max_iter <- max_iter # max iterations before terminating if mininum isn't found
  tolerance <- tol # tolerance for the stoppign criteria 
  obj_values <- numeric(max_iter) # Stores the value of f(x)
  eta_values <- numeric(max_iter)  # To store eta values used each iteration
  betas <- numeric(max_iter) # Stores the value of x guesses
  x0 <- runif(1, min=-10, max=10) # our first guess is somewhere between -10-10
  
  # Set the objective function to the function to be minimized 
  # Objective function: f(x)
  obj_function <- function(x) {
    return(x^4 + 2*(x^2) + 1) 
  }
  
  # Gradient function: d/dx of f(x)
  gradient <- function(x) {
    return(4*x^3 + 4*x)
  }
  
  # Append the first guess to the obj_values and betas vector
  betas[1] <- x0
  obj_values[1] <- obj_function(x0)
  
  # Step 2: While stopping criteria < tolerance, do:
  for (iter in 1:max_iter) { # the iteration goes n = 1, 2, 3, 4, but the arrays of our output starts at iter = 0 and guess x0
    # Select eta(step size), which is constant
    # There's nothing to do for this step
    
    # Calculate the next guess of x_k+1, calculate f(x_k+1), set eta(x_k+1)
    betas[iter + 1] <- betas[iter] - (eta * gradient(betas[iter]))
    obj_values[iter + 1] <- obj_function(betas[iter + 1])
    eta_values[iter + 1] <- eta
    
    # Calculate the value of the stopping criterion
    stop_criteria <- abs(gradient(betas[iter + 1]))
    
    # If stopping criteria less than tolerance, break
    if(is.na(stop_criteria) || stop_criteria <= tolerance) { 
      last_iter <- iter + 1
      break 
    }
    
    # if we never set last iter, then we hit the max number of iterations and need to set
    if(last_iter == 0) { last_iter <- max_iter }
    
    # end algorithm
  }
  
  return(list(betas = betas, obj_values = obj_values, eta_values = eta_values, last_iter = last_iter)) # in this case, beta(predictors) are the x values, obj_values are f(x), eta is the step size, last iter is the value in the vector of the final iteration before stopping
}
```

Running the gradient descent algorithm with fixed step size:

```{r minimizing_constant_step}
minimize_constant_step <- gradient_descent_constant_step(tol = 1e-6, max_iter = 10000, step_size = 0.01)
print(minimize_constant_step)

cat("The functions stopped after", minimize_constant_step$last_iter - 1, "iterations \n")
cat("The function's point of minimization is", "(", minimize_constant_step$betas[minimize_constant_step$last_iter], "," , minimize_constant_step$obj_values[minimize_constant_step$last_iter], ") \n")
```

**Backtracking Line Search is implemented as follows:**  

1. Select a random starting point $x_0$    
2. While stopping criteria < tolerance, do:  

  * Select $η_k$ using backtracking line search
  * Calculate $x_{(k+1)} = x_k - η_k * ∇(f(x_k))$  
  * Calculate the value of stopping criterion  
  
Backtracking Line Search:  

  * Set $η^0 > 0$(usually a large value), $ϵ ∈ (0,1)$ and $τ ∈ (0,1)$
  * Set $η_1 = η^0$ 
  * At iteration k, set $η_k <- η_{k-1}$
    1. Check whether the Armijo Condition holds: 
    $$
    h(η_k) ≤ h(0) + ϵη_kh'(0)
    $$  
      where $h(η_k) = f(x_k) − η_k ∇f(x_k)$
    2. 
      + If yes(condition holds), terminate and keep $η_k$
      + If no, set $η_k = τη_k$ and go to Step 1

Stopping criteria: Stop if $∣|∇(f(x_k)||_2 ≤ ϵ$ 

Other note: Since we need h'(0) for the Armijo condition calculation, that is given by:  
$$
h'(0) = -[∇f(x_k)]^\top ∇f(x_k)
$$
Since we are minimizing x, we have a one dimensional beta, we can simplify to  
$$
h'(0) = -||∇f(x_k)||^2
$$

To summarize, backtracking line search chooses the step size by ensuring the Armijo condition always holds. If the Armijo condition doesn't hold, we are probably overshooting, hence the step size gets updated iteratively 

```{r backtracking_line_search_grad_descent}
gradient_descent_backtracking <- function(tol = 1e-6, max_iter = 10000, epsilon = 0.5, tau = 0.5, init_step_size = 0.01) {
  # Step 1: Initialize and select a random stopping point
  # Initialize
  set.seed(777) # example seeding 
  last_iter <- 0 # the last iteration ran
  max_iter <- max_iter # max iterations before terminating if minimum isn't found
  tolerance <- tol # tolerance for the stopping criteria 
  epsilon <- epsilon # Epsilon used in the step size criteria calculation
  tau <- tau # tau used in the step size criteria calculation
  obj_values <- numeric(max_iter) # Stores the value of f(x)
  eta_values <- numeric(max_iter)  # To store eta values used each iteration
  betas <- numeric(max_iter) # Stores the value of x guesses
  x0 <- runif(1, min=-10, max=10) # our first guess is somewhere between -10 to 10
  eta <- init_step_size # our initial step size
  
  # Set the objective function to the function to be minimized 
  # Objective function: f(x)
  obj_function <- function(x) {
    return(x^4 + 2*(x^2) + 1) 
  }
  
  # Gradient function: d/dx of f(x)
  gradient <- function(x) {
    return(4*x^3 + 4*x)
  }
  
  # Armijo condition function
  # returns TRUE or FALSE whether the condition is satisfied or not
  calc_armijo_stepsize <- function(beta_new, beta, eta) {
    iter <- 1 # set a hard limit of 10000 iterations
    while (obj_function(beta_new) > (obj_function(beta) - (epsilon * eta * gradient(beta)^2)) && iter <= max_iter) {
      eta <- tau * eta
      iter <- iter + 1
    }
    return(eta)
  }
  
  # Append the first guess to the obj_values and betas vector
  betas[1] <- x0
  obj_values[1] <- obj_function(x0)
  
  # Step 2: While stopping criteria < tolerance, do:
  for (iter in 1:max_iter) { # the iteration goes n = 1, 2, 3, 4, but the arrays of our output starts at iter = 0 and guess x0
    
    # Calculate the next guess of x_k+1, calculate f(x_k+1)
    beta <- betas[iter]
    beta_new <- betas[iter] - (eta * gradient(beta))
    betas[iter + 1] <- beta_new
    new_obj <- obj_function(beta_new)
    obj_values[iter + 1] <- new_obj
    
    # Select eta(step size) for next step(k+1) using backtracking line search
    eta <- calc_armijo_stepsize(beta_new = beta_new, beta = beta, eta = eta)
    eta_values[iter + 1] <- eta
    
    # Calculate the value of the stopping criterion
    stop_criteria <- abs(gradient(beta_new))
    
    # If stopping criteria less than tolerance, break
    if(is.na(stop_criteria) || stop_criteria <= tolerance) { 
      last_iter <- iter + 1
      break 
    }
    
    # if we never set last iter, then we hit the max number of iterations and need to set
    if(last_iter == 0) { last_iter <- max_iter }
    
    # end algorithm
  }
  
  return(list(betas = betas, obj_values = obj_values, eta_values = eta_values, last_iter = last_iter)) # in this case, beta(predictors) are the x values, obj_values are f(x), eta is the step size, last iter is the value in the vector of the final iteration before stopping
}
```

Running the gradient descent algorithm with backtracking:

```{r minimizing_constant_step}
minimize_backtrack <- gradient_descent_backtracking(tol = 1e-6, max_iter = 10000, epsilon = 0.5, tau = 0.8, init_step_size = 0.01)
print(minimize_backtrack)

cat("The functions stopped after", minimize_backtrack$last_iter - 1, "iterations \n")
cat("The function's point of minimization is", "(", minimize_backtrack$betas[minimize_backtrack$last_iter], "," , minimize_backtrack$obj_values[minimize_backtrack$last_iter], ") \n")
```

### 1) For the constant step size version of gradient descent, discuss how you selected the step size used in your code

Theoretical Analysis proves that for functions with a unique global minimum, the step size should be within 0 to 1/L to converge to the unique global minimum, where L is the Lipchitz constant, given by:  
$$
||∇f(x) - ∇f(y)||_2 \le L||x - y||_2
$$


### 2) For both versions of the gradient descent algorithm, plot the value of $f(x_k)$ as a function of k the number of iterations

```{r}
# constant step size
iterations <- 1:minimize_constant_step$last_iter
obj_values <- (minimize_constant_step$obj_values)[iterations]
f_k_constant <- cbind(obj_values, iterations)

```


### 3) or the the gradient descent method with backtracking line search, plot the step size $η_k$ selected at step k as a function of k. Comment on the result

