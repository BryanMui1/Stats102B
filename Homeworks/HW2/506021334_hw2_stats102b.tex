% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{fvextra}
\usepackage{unicode-math}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
\DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\{\}}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={HW 2},
  pdfauthor={Bryan Mui - UID 506021334 - 28 April 2025},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{HW 2}
\author{Bryan Mui - UID 506021334 - 28 April 2025}
\date{}

\begin{document}
\maketitle


Loaded packages: ggplot2, tidyverse (include = false for this chunk)

Reading the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"dataset{-}logistic{-}regression.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 10000 Columns: 101
-- Column specification --------------------------------------------------------
Delimiter: ","
dbl (101): y, X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X...

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(data, }\AttributeTok{n =} \DecValTok{25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 25 x 101
       y      X1      X2     X3     X4     X5     X6      X7     X8      X9
   <dbl>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl>   <dbl>
 1     1 -0.0895  0.450   1.71   0.657 -0.392  1.24   0.895   1.13  -0.0117
 2     1 -0.0943  0.281  -0.147 -0.701  0.400 -0.210  0.677  -0.440  0.458 
 3     0 -0.431  -0.445  -0.777 -0.832 -2.26  -1.62  -1.98   -1.67  -1.15  
 4     0  0.644   0.0817 -0.448  0.852 -1.02   0.671  0.299   0.145 -0.205 
 5     1 -0.919  -0.0241  0.807 -0.612 -0.498  0.350  1.12    0.242 -0.947 
 6     0 -1.89   -1.11   -0.210  0.161 -1.34  -2.04  -0.0135 -1.39  -1.31  
 7     0 -1.34   -0.804   0.322 -0.110  0.624 -0.329 -0.432  -0.191  0.171 
 8     1  0.329   0.468   0.719  0.588  1.71   1.39   0.603   0.650  0.161 
 9     0  0.332   1.42   -0.431  1.02   0.484  0.348  0.474   1.26  -0.479 
10     0 -0.311   0.0193  0.168 -0.346  0.626 -0.704 -0.290   0.680 -0.0453
# i 15 more rows
# i 91 more variables: X10 <dbl>, X11 <dbl>, X12 <dbl>, X13 <dbl>, X14 <dbl>,
#   X15 <dbl>, X16 <dbl>, X17 <dbl>, X18 <dbl>, X19 <dbl>, X20 <dbl>,
#   X21 <dbl>, X22 <dbl>, X23 <dbl>, X24 <dbl>, X25 <dbl>, X26 <dbl>,
#   X27 <dbl>, X28 <dbl>, X29 <dbl>, X30 <dbl>, X31 <dbl>, X32 <dbl>,
#   X33 <dbl>, X34 <dbl>, X35 <dbl>, X36 <dbl>, X37 <dbl>, X38 <dbl>,
#   X39 <dbl>, X40 <dbl>, X41 <dbl>, X42 <dbl>, X43 <dbl>, X44 <dbl>, ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{777}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Our data set has 10000 observations, 1 binary outcome variable y, and
100 predictor variables X1-X100

Separating into X matrix and y vector:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{y)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\section{Problem 1}\label{problem-1}

\subsection{\texorpdfstring{Part
(\(\symbf{\alpha}\))}{Part (\textbackslash symbf\{\textbackslash alpha\})}}\label{part-symbfalpha}

The optimization problem is to minimize the log-likelihood function.
From there we will get the objective function and gradient function

From the slides in class we have:

\[
\min_{\beta} (-\ell(\beta)) = \frac{1}{m} \sum_{i=1}^{m} f_i(\beta)
\]

and the equation for \(f_i(\beta)\):

\[
f_i(\beta) = -y_i(x_i^{\mathsf{T}}\beta) + log(1 + exp(x_i^{\mathsf{T}} \beta))
\] For the objective function, we get:

\[
f(\beta) = \frac{1}{m} \sum_{i=1}^{m} [-y_i(x_i^{\mathsf{T}}\beta) + log(1 + exp(x_i^{\mathsf{T}} \beta))]
\]

We can matricize the objective function to

\[
\boxed{f(\beta) = \frac{1}{m}[-y^{\mathsf{T}}(X\beta) + \mathbf{1}^{\mathsf{T}}log(1 + exp(X\beta))]}
\]

We also have the gradient function:

\[
\nabla f(x) = \frac{1}{m} \sum_{i=1}^m \nabla f_i(x)
\]

and

\[
\nabla_\beta f_i(\beta) = [\sigma(x_i^{\mathsf{T}} \beta) - y_i] \cdot x_i
\] where \(\sigma(z) = \frac{1}{1+exp(-z)}\) as the logistic sigmoid
function, therefore:

\[
\begin{aligned}
\nabla f(x) &= \frac{1}{m} \sum_{i=1}^m \nabla f_i(x), \; \nabla_\beta f_i(\beta) = [\sigma(x_i^{\mathsf{T}} \beta) - y_i] \cdot x_i \\
\nabla f(\beta) &= {\frac{1}{m} \sum_{i=1}^m [\sigma(x_i^{\mathsf{T}} \beta) - y_i] \cdot x_i}
\end{aligned}
\]

And we can also matricize this:

\[
\boxed{\nabla f(\beta) = \frac{1}{m} X^{\mathsf{T}}[\sigma(X\beta) - y], \quad \sigma(z) = \frac{1}{1+exp(-z)}}
\]

Therefore our gradient descent update step is(for constant step size):

\[
\boxed {\beta_{k+1} = \beta_k - \eta \nabla f(\beta_k)}
\]

\textbf{Implement the following algorithms to obtain estimates of the
regression coefficients \(\symbf{β}\):}

\subsubsection{(1) Gradient descent with backtracking line
search}\label{gradient-descent-with-backtracking-line-search}

Algorithm; Backtracking Line Search:

Params:

\begin{itemize}
\tightlist
\item
  Set \(η^0 > 0\)(usually a large value \textasciitilde1),
\item
  Set \(η_1 = η^0\)
\item
  Set \(ϵ ∈ (0,1), τ ∈ (0,1)\), where \(ϵ\) and \(τ\) are used to modify
  step size
\end{itemize}

Repeat:

\begin{itemize}
\tightlist
\item
  At iteration k, set \(η_k <- η_{k-1}\)

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    Check whether the Armijo Condition holds: \[
    h(η_k) ≤ h(0) + ϵη_kh'(0)
    \]\\
    where \(h(η_k) = f(x_k) − η_k ∇f(x_k)\),\\
    and \(h(0) = f(x_k)\),\\
    and \(h'(0) = -||\nabla (x_k)||^2\)
  \item
  \end{enumerate}

  \begin{itemize}
  \tightlist
  \item
    If yes(condition holds), terminate and keep \(η_k\)
  \item
    If no, set \(η_k = τη_k\) and go to Step 1
  \end{itemize}
\end{itemize}

Stopping criteria: Stop if \(||x_k - x_{k+1}|| ≤ tol\) (change in
parameters is small)

\textbf{Implement BLS}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# logistic gradient descent w/ bls}
\NormalTok{log\_bls }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(X, y, }\AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{, }\AttributeTok{max\_iter =} \DecValTok{10000}\NormalTok{, }\AttributeTok{epsilon =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{tau =} \FloatTok{0.5}\NormalTok{) \{}
  \CommentTok{\# Initialize}
\NormalTok{  n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(X)}
\NormalTok{  p }\OtherTok{\textless{}{-}} \FunctionTok{ncol}\NormalTok{(X)}
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(X)}
\NormalTok{  y }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(y)}
\NormalTok{  beta }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, p))}
\NormalTok{  obj\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(max\_iter)}
\NormalTok{  eta\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(max\_iter)  }\CommentTok{\# To store eta values used each iteration}
\NormalTok{  beta\_values }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{() }\CommentTok{\# To store beta values used each iteration}
\NormalTok{  eta\_bt }\OtherTok{\textless{}{-}} \DecValTok{1}  \CommentTok{\# Initial step size for backtracking}
  
  \CommentTok{\# Objective function: negative log{-}likelihood}
  \CommentTok{\# input: Beta vector, x matrix, y matrix}
  \CommentTok{\# output: scalar objective func value}
  \CommentTok{\# comments: We want to minimize this function for logit regression}
\NormalTok{  obj\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, x, y) \{}
\NormalTok{    m }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(x)}
\NormalTok{    z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{\%*\%}\NormalTok{ beta}
\NormalTok{    (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ m) }\SpecialCharTok{*}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{(}\FunctionTok{t}\NormalTok{(y) }\SpecialCharTok{\%*\%}\NormalTok{ z) }\SpecialCharTok{+} \FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(z))))}
\NormalTok{  \}}
  
  \CommentTok{\# Gradient function}
  \CommentTok{\# input: Beta vector, x matrix, y matrix}
  \CommentTok{\# output: gradient vector in the dimension of nrow(Beta) x 1}
  \CommentTok{\# comments: We use this for gradient descent}
\NormalTok{  gradient }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, x, y) \{}
\NormalTok{    m }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(x)                       }\CommentTok{\# define m}
\NormalTok{    sig }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(z) }\DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{z))  }\CommentTok{\# sigmoid function}
\NormalTok{    (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ m) }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{t}\NormalTok{(x) }\SpecialCharTok{\%*\%}\NormalTok{ (}\FunctionTok{sig}\NormalTok{(x }\SpecialCharTok{\%*\%}\NormalTok{ beta) }\SpecialCharTok{{-}}\NormalTok{ y))}
\NormalTok{  \}}

  \CommentTok{\# Algorithm:}
  \ControlFlowTok{for}\NormalTok{ (iter }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{max\_iter) \{}
\NormalTok{    grad }\OtherTok{\textless{}{-}} \FunctionTok{gradient}\NormalTok{(beta, x, y)}
    
    \CommentTok{\#cat("iter ", iter, "\textbackslash{}n")}
    
    \CommentTok{\# backtracking step}
\NormalTok{    current\_obj }\OtherTok{\textless{}{-}} \FunctionTok{obj\_function}\NormalTok{(beta, x, y)}
\NormalTok{    grad\_norm\_sq }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(grad}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
    
\NormalTok{    beta\_new }\OtherTok{\textless{}{-}}\NormalTok{ beta }\SpecialCharTok{{-}}\NormalTok{ eta\_bt }\SpecialCharTok{*}\NormalTok{ grad}
    
    \ControlFlowTok{while}\NormalTok{ (}\FunctionTok{obj\_function}\NormalTok{(beta\_new, x, y) }\SpecialCharTok{\textgreater{}}\NormalTok{ current\_obj }\SpecialCharTok{{-}}\NormalTok{ epsilon }\SpecialCharTok{*}\NormalTok{ eta\_bt }\SpecialCharTok{*}\NormalTok{ grad\_norm\_sq) \{}
\NormalTok{      eta\_bt }\OtherTok{\textless{}{-}}\NormalTok{ tau }\SpecialCharTok{*}\NormalTok{ eta\_bt}
\NormalTok{      beta\_new }\OtherTok{\textless{}{-}}\NormalTok{ beta }\SpecialCharTok{{-}}\NormalTok{ eta\_bt }\SpecialCharTok{*}\NormalTok{ grad}
\NormalTok{    \}}
    
    \CommentTok{\# save values to the matrix}
\NormalTok{    eta\_values[iter] }\OtherTok{\textless{}{-}}\NormalTok{ eta\_bt}
\NormalTok{    obj\_values[iter] }\OtherTok{\textless{}{-}} \FunctionTok{obj\_function}\NormalTok{(beta\_new, x, y)}
\NormalTok{    beta\_values[[iter]] }\OtherTok{\textless{}{-}}\NormalTok{ beta\_new}
    
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{((beta\_new }\SpecialCharTok{{-}}\NormalTok{ beta)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\textless{}}\NormalTok{ tol) \{}
      \CommentTok{\# set the vector ranges and break}
\NormalTok{      beta }\OtherTok{\textless{}{-}}\NormalTok{ beta\_new}
\NormalTok{      obj\_values }\OtherTok{\textless{}{-}}\NormalTok{ obj\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{iter]}
\NormalTok{      eta\_values }\OtherTok{\textless{}{-}}\NormalTok{ eta\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{iter]}
\NormalTok{      beta\_values }\OtherTok{\textless{}{-}}\NormalTok{ beta\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{iter]}
      \ControlFlowTok{break}
\NormalTok{    \}}
    
\NormalTok{    beta }\OtherTok{\textless{}{-}}\NormalTok{ beta\_new}
\NormalTok{  \}}
  
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{beta =}\NormalTok{ beta, }\AttributeTok{obj\_values =}\NormalTok{ obj\_values, }\AttributeTok{eta\_values =}\NormalTok{ eta\_values, }\AttributeTok{beta\_values =}\NormalTok{ beta\_values))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\textbf{TESTING: BLS}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log\_reg\_bls }\OtherTok{\textless{}{-}} \FunctionTok{log\_bls}\NormalTok{(X, y, }\AttributeTok{tol=}\FloatTok{1e{-}6}\NormalTok{, }\AttributeTok{max\_iter=}\DecValTok{10000}\NormalTok{, }\AttributeTok{epsilon=}\FloatTok{0.5}\NormalTok{, }\AttributeTok{tau=}\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"betas }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
betas 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(log\_reg\_bls}\SpecialCharTok{$}\NormalTok{beta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                 y
X1   -0.1418188273
X2   -0.0601340162
X3    0.1588169528
X4    0.1328223189
X5   -0.0480437781
X6    0.0992481092
X7    0.1189707785
X8    0.1165560855
X9    0.0121222291
X10   0.0002641372
X11   0.0440526577
X12  -0.1793886158
X13  -0.0107332284
X14  -0.1230510680
X15   0.0724799230
X16   0.0571868940
X17   0.1299458439
X18   0.1249113906
X19  -0.0018170795
X20   0.1248825007
X21  -0.0107845610
X22  -0.1431801553
X23  -0.1094846603
X24   0.0576435159
X25  -0.1190174922
X26   0.0164879978
X27  -0.0977482724
X28   0.1544632196
X29  -0.0276524076
X30   0.0164226883
X31  -0.0589010945
X32   0.0205242099
X33   0.1352153619
X34  -0.0301792708
X35  -0.0097106467
X36   0.0631274232
X37   0.1972595891
X38   0.0932479560
X39   0.1242393813
X40   0.1466042152
X41   0.1112967707
X42  -0.1226544766
X43  -0.0374866338
X44  -0.0155583465
X45  -0.0103256878
X46  -0.1807311531
X47   0.0122916067
X48   0.0309436582
X49   0.0257891274
X50   0.1230837280
X51  -0.0237134869
X52  -0.0136672407
X53   0.0802510780
X54   0.1695795679
X55   0.1711403640
X56  -0.0447703054
X57  -0.0407325139
X58  -0.0768578382
X59   0.0786448045
X60  -0.1192193182
X61  -0.0080431756
X62   0.0701535429
X63   0.0295238798
X64  -0.1090225592
X65   0.0633967271
X66  -0.1450871355
X67   0.1404424947
X68   0.0649021774
X69  -0.1595801011
X70   0.1128079446
X71   0.1888668197
X72   0.0920649207
X73  -0.0647758044
X74  -0.0684344716
X75   0.2306707321
X76  -0.1312078759
X77   0.0301767178
X78  -0.0742090881
X79   0.0695790861
X80  -0.0273839196
X81   0.0183730389
X82   0.0555339156
X83  -0.0196159895
X84  -0.0119020076
X85   0.0981161430
X86   0.1724354285
X87   0.0832570899
X88  -0.0070115810
X89   0.0720539875
X90   0.0779093972
X91   0.0026928031
X92  -0.1223692130
X93   0.0073627318
X94  -0.0996425700
X95  -0.0485788118
X96   0.0338587696
X97   0.1496954257
X98   0.1702285222
X99   0.0197714549
X100  0.0070161693
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"The function converged after"}\NormalTok{, }\FunctionTok{length}\NormalTok{(log\_reg\_bls}\SpecialCharTok{$}\NormalTok{obj\_values), }\StringTok{" iterations }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The function converged after 1909  iterations 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"Eta Vals: }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Eta Vals: 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(log\_reg\_bls}\SpecialCharTok{$}\NormalTok{eta\_values[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625
[11] 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625
[21] 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625
[31] 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625
[41] 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625 0.0625
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"Objective Function vals }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Objective Function vals 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(log\_reg\_bls}\SpecialCharTok{$}\NormalTok{obj\_values[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 0.5642463 0.5491551 0.5446388 0.5427888 0.5418291 0.5412092 0.5407301
 [8] 0.5403132 0.5399257 0.5395537 0.5391908 0.5388343 0.5384829 0.5381361
[15] 0.5377934 0.5374547 0.5371200 0.5367892 0.5364622 0.5361389 0.5358194
[22] 0.5355035 0.5351913 0.5348826 0.5345775 0.5342759 0.5339777 0.5336830
[29] 0.5333916 0.5331036 0.5328188 0.5325373 0.5322591 0.5319840 0.5317120
[36] 0.5314432 0.5311774 0.5309146 0.5306549 0.5303981 0.5301442 0.5298932
[43] 0.5296451 0.5293997 0.5291572 0.5289174 0.5286804 0.5284460 0.5282142
[50] 0.5279851
\end{verbatim}

\subsubsection{(2) Gradient descent with backtracking line search and
Nesterov
momentum}\label{gradient-descent-with-backtracking-line-search-and-nesterov-momentum}

Nesterov is simply BLS with a special way to select the momentum
\(\xi\),

We set \(\xi\) to:

\[
\frac{k-1}{k+2}
\]

where k is the iteration index

Algorithm(Nesterov Momentum with BLS)

Params:

\begin{itemize}
\tightlist
\item
  Set \(η^0 > 0\)(usually a large value \textasciitilde1),
\item
  Set \(η_1 = η^0\)
\item
  Set \(ϵ ∈ (0,1), τ ∈ (0,1)\), where \(ϵ\) and \(τ\) are used to modify
  step size
\end{itemize}

Repeat:

\begin{itemize}
\tightlist
\item
  At iteration k, set \(η_k <- η_{k-1}\), update with
\end{itemize}

\[
\boxed{x_{k+1} = y_k - \eta_k \nabla (f(y_k)), \quad y_k = x_k + \xi(x_k - x_{k-1}), \quad \xi = \frac{k-1}{k+2}}
\]

\begin{itemize}
\tightlist
\item
  Check the next setting of \(\eta\):

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Check whether the Armijo Condition holds:
  \end{enumerate}

  \[
  h(η_k) ≤ h(0) + ϵη_kh'(0)
  \]\\
  where \(h(η_k) = f(x_k) − η_k ∇f(x_k)\),\\
  and \(h(0) = f(x_k)\),\\
  and \(h'(0) = -||\nabla (x_k)||^2\)

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{1}
  \tightlist
  \item
  \end{enumerate}

  \begin{itemize}
  \tightlist
  \item
    If yes(condition holds), terminate and keep \(η_k\)
  \item
    If no, set \(η_k = τη_k\) and go to Step 1
  \end{itemize}
\end{itemize}

Stopping criteria: Stop if \(||x_k - x_{k+1}|| ≤ tol\) (change in
parameters is small)

\textbf{Implement BLS Nesterov}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# logistic gradient descent w/ bls nesterov}
\NormalTok{log\_bls\_n }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(X, y, }\AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{, }\AttributeTok{max\_iter =} \DecValTok{10000}\NormalTok{, }\AttributeTok{epsilon =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{tau =} \FloatTok{0.8}\NormalTok{) \{}
  \CommentTok{\# Initialize}
\NormalTok{  n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(X)}
\NormalTok{  p }\OtherTok{\textless{}{-}} \FunctionTok{ncol}\NormalTok{(X)}
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(X)}
\NormalTok{  y }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(y)}
\NormalTok{  beta }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, p))}
\NormalTok{  obj\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(max\_iter)}
\NormalTok{  eta\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(max\_iter)  }\CommentTok{\# To store eta values used each iteration}
\NormalTok{  beta\_values }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{() }\CommentTok{\# To store beta values used each iteration}
\NormalTok{  eta\_bt }\OtherTok{\textless{}{-}} \DecValTok{1}  \CommentTok{\# Initial step size for backtracking}
  
  \CommentTok{\# Objective function: negative log{-}likelihood}
  \CommentTok{\# input: Beta vector, x matrix, y matrix}
  \CommentTok{\# output: scalar objective func value}
  \CommentTok{\# comments: We want to minimize this function for logit regression}
\NormalTok{  obj\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, x, y) \{}
\NormalTok{    m }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(x)}
\NormalTok{    z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{\%*\%}\NormalTok{ beta}
\NormalTok{    (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ m) }\SpecialCharTok{*}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{(}\FunctionTok{t}\NormalTok{(y) }\SpecialCharTok{\%*\%}\NormalTok{ z) }\SpecialCharTok{+} \FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(z))))}
\NormalTok{  \}}
  
  \CommentTok{\# Gradient function}
  \CommentTok{\# input: Beta vector, x matrix, y matrix}
  \CommentTok{\# output: gradient vector in the dimension of nrow(Beta) x 1}
  \CommentTok{\# comments: We use this for gradient descent}
\NormalTok{  gradient }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, x, y) \{}
\NormalTok{    m }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(x)                       }\CommentTok{\# define m}
\NormalTok{    sig }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(z) }\DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{z))  }\CommentTok{\# sigmoid function}
\NormalTok{    (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ m) }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{t}\NormalTok{(x) }\SpecialCharTok{\%*\%}\NormalTok{ (}\FunctionTok{sig}\NormalTok{(x }\SpecialCharTok{\%*\%}\NormalTok{ beta) }\SpecialCharTok{{-}}\NormalTok{ y))}
\NormalTok{  \}}

  \CommentTok{\# Algorithm:}
  \ControlFlowTok{for}\NormalTok{ (iter }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{max\_iter) \{}
\NormalTok{    grad }\OtherTok{\textless{}{-}} \FunctionTok{gradient}\NormalTok{(beta, x, y)}
    
    \CommentTok{\#cat("iter ", iter, "\textbackslash{}n")}
    
    \CommentTok{\# backtracking step}
\NormalTok{    current\_obj }\OtherTok{\textless{}{-}} \FunctionTok{obj\_function}\NormalTok{(beta, x, y)}
\NormalTok{    grad\_norm\_sq }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(grad}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
    
    \ControlFlowTok{if}\NormalTok{(iter }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) \{}
\NormalTok{      eta\_bt }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{      y\_k }\OtherTok{\textless{}{-}}\NormalTok{ beta}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{      beta\_prev }\OtherTok{\textless{}{-}}\NormalTok{ beta\_values[[iter }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{]]}
\NormalTok{      xi }\OtherTok{\textless{}{-}}\NormalTok{ (iter }\SpecialCharTok{+} \DecValTok{1}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (iter }\SpecialCharTok{+} \DecValTok{2}\NormalTok{)}
\NormalTok{      y\_k }\OtherTok{\textless{}{-}}\NormalTok{ beta }\SpecialCharTok{+}\NormalTok{ xi }\SpecialCharTok{*}\NormalTok{ ((beta }\SpecialCharTok{{-}}\NormalTok{ beta\_prev))}
\NormalTok{    \}}

\NormalTok{    beta\_new }\OtherTok{\textless{}{-}}\NormalTok{ y\_k }\SpecialCharTok{{-}}\NormalTok{ eta\_bt }\SpecialCharTok{*}\NormalTok{ grad}
    
    \ControlFlowTok{while}\NormalTok{ (}\FunctionTok{obj\_function}\NormalTok{(beta\_new, x, y) }\SpecialCharTok{\textgreater{}}\NormalTok{ current\_obj }\SpecialCharTok{{-}}\NormalTok{ epsilon }\SpecialCharTok{*}\NormalTok{ eta\_bt }\SpecialCharTok{*}\NormalTok{ grad\_norm\_sq) \{}
\NormalTok{      eta\_bt }\OtherTok{\textless{}{-}}\NormalTok{ tau }\SpecialCharTok{*}\NormalTok{ eta\_bt}
\NormalTok{      beta\_new }\OtherTok{\textless{}{-}}\NormalTok{ beta }\SpecialCharTok{{-}}\NormalTok{ eta\_bt }\SpecialCharTok{*}\NormalTok{ grad}
\NormalTok{    \}}
    
    \CommentTok{\# save values to the matrix}
\NormalTok{    eta\_values[iter] }\OtherTok{\textless{}{-}}\NormalTok{ eta\_bt}
\NormalTok{    obj\_values[iter] }\OtherTok{\textless{}{-}} \FunctionTok{obj\_function}\NormalTok{(beta\_new, x, y)}
\NormalTok{    beta\_values[[iter]] }\OtherTok{\textless{}{-}}\NormalTok{ beta\_new}
    
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{((beta\_new }\SpecialCharTok{{-}}\NormalTok{ beta)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\textless{}}\NormalTok{ tol) \{}
      \CommentTok{\# set the vector ranges and break}
\NormalTok{      beta }\OtherTok{\textless{}{-}}\NormalTok{ beta\_new}
\NormalTok{      obj\_values }\OtherTok{\textless{}{-}}\NormalTok{ obj\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{iter]}
\NormalTok{      eta\_values }\OtherTok{\textless{}{-}}\NormalTok{ eta\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{iter]}
\NormalTok{      beta\_values }\OtherTok{\textless{}{-}}\NormalTok{ beta\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{iter]}
      \ControlFlowTok{break}
\NormalTok{    \}}
    
\NormalTok{    beta }\OtherTok{\textless{}{-}}\NormalTok{ beta\_new}
\NormalTok{  \}}
  
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{beta =}\NormalTok{ beta, }\AttributeTok{obj\_values =}\NormalTok{ obj\_values, }\AttributeTok{eta\_values =}\NormalTok{ eta\_values, }\AttributeTok{beta\_values =}\NormalTok{ beta\_values))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\textbf{TESTING: BLS}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log\_reg\_bls\_n }\OtherTok{\textless{}{-}} \FunctionTok{log\_bls\_n}\NormalTok{(X, y, }\AttributeTok{tol=}\FloatTok{1e{-}6}\NormalTok{, }\AttributeTok{max\_iter=}\DecValTok{10000}\NormalTok{, }\AttributeTok{epsilon=}\FloatTok{0.5}\NormalTok{, }\AttributeTok{tau=}\FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{PRINTING OUTPUT}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"betas }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
betas 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(log\_reg\_bls\_n}\SpecialCharTok{$}\NormalTok{beta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                 y
X1   -0.1418263794
X2   -0.0601388591
X3    0.1588267497
X4    0.1328279392
X5   -0.0480506394
X6    0.0992548055
X7    0.1189794773
X8    0.1165617782
X9    0.0121210900
X10   0.0002652682
X11   0.0440525896
X12  -0.1794038315
X13  -0.0107339373
X14  -0.1230560308
X15   0.0724838710
X16   0.0571919899
X17   0.1299511146
X18   0.1249158303
X19  -0.0018154954
X20   0.1248931359
X21  -0.0107871202
X22  -0.1431957846
X23  -0.1094930269
X24   0.0576439569
X25  -0.1190248307
X26   0.0164864074
X27  -0.0977571692
X28   0.1544691126
X29  -0.0276528501
X30   0.0164184648
X31  -0.0589073210
X32   0.0205289418
X33   0.1352223830
X34  -0.0301846415
X35  -0.0097152706
X36   0.0631324820
X37   0.1972701788
X38   0.0932518671
X39   0.1242469201
X40   0.1466077067
X41   0.1113013302
X42  -0.1226607166
X43  -0.0374935885
X44  -0.0155599449
X45  -0.0103222178
X46  -0.1807432316
X47   0.0122929903
X48   0.0309476137
X49   0.0257875173
X50   0.1230898353
X51  -0.0237110270
X52  -0.0136673590
X53   0.0802556361
X54   0.1695903968
X55   0.1711505641
X56  -0.0447755635
X57  -0.0407377293
X58  -0.0768652234
X59   0.0786463310
X60  -0.1192273578
X61  -0.0080502803
X62   0.0701567008
X63   0.0295264284
X64  -0.1090289520
X65   0.0633998643
X66  -0.1450928496
X67   0.1404497219
X68   0.0649048905
X69  -0.1595896445
X70   0.1128140054
X71   0.1888821288
X72   0.0920697212
X73  -0.0647787849
X74  -0.0684414958
X75   0.2306799570
X76  -0.1312182054
X77   0.0301762811
X78  -0.0742102167
X79   0.0695810289
X80  -0.0273871826
X81   0.0183748140
X82   0.0555414970
X83  -0.0196157885
X84  -0.0119065906
X85   0.0981180432
X86   0.1724480190
X87   0.0832605298
X88  -0.0070189478
X89   0.0720560185
X90   0.0779116425
X91   0.0026900656
X92  -0.1223807711
X93   0.0073624203
X94  -0.0996496631
X95  -0.0485854423
X96   0.0338563238
X97   0.1497058189
X98   0.1702384964
X99   0.0197774565
X100  0.0070126286
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"The function converged after"}\NormalTok{, }\FunctionTok{length}\NormalTok{(log\_reg\_bls\_n}\SpecialCharTok{$}\NormalTok{obj\_values), }\StringTok{" iterations }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The function converged after 1443  iterations 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"Eta Vals: }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Eta Vals: 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(log\_reg\_bls\_n}\SpecialCharTok{$}\NormalTok{eta\_values[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 0.08589935 0.08589935 0.08589935 0.08589935 0.08589935 0.08589935
 [7] 0.08589935 0.08589935 0.08589935 0.08589935 0.08589935 0.08589935
[13] 0.08589935 0.08589935 0.08589935 0.08589935 0.08589935 0.08589935
[19] 0.08589935 0.08589935 0.08589935 0.08589935 0.08589935 0.08589935
[25] 0.08589935 0.08589935 0.08589935 0.08589935 0.08589935 0.08589935
[31] 0.08589935 0.08589935 0.08589935 0.08589935 0.08589935 0.08589935
[37] 0.08589935 0.08589935 0.08589935 0.08589935 0.08589935 0.08589935
[43] 0.08589935 0.08589935 0.08589935 0.08589935 0.08589935 0.08589935
[49] 0.08589935 0.08589935
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"Objective Function vals }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Objective Function vals 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(log\_reg\_bls\_n}\SpecialCharTok{$}\NormalTok{obj\_values[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 0.5475063 0.5433692 0.5420351 0.5412959 0.5407164 0.5401878 0.5396798
 [8] 0.5391836 0.5386963 0.5382171 0.5377455 0.5372814 0.5368247 0.5363752
[15] 0.5359328 0.5354973 0.5350688 0.5346469 0.5342317 0.5338231 0.5334209
[22] 0.5330250 0.5326353 0.5322517 0.5318741 0.5315025 0.5311367 0.5307766
[29] 0.5304221 0.5300732 0.5297297 0.5293916 0.5290587 0.5287310 0.5284085
[36] 0.5280909 0.5277783 0.5274705 0.5271675 0.5268692 0.5265755 0.5262864
[43] 0.5260017 0.5257214 0.5254454 0.5251737 0.5249062 0.5246428 0.5243834
[50] 0.5241280
\end{verbatim}

\subsubsection{(3) Gradient descent with AMSGrad-ADAM
momentum}\label{gradient-descent-with-amsgrad-adam-momentum}

(no backtracking line search, since AMSGrad-ADAM adjusts step sizes per
parameter using momentum and adaptive scaling)

AMSGrad-ADAM is a special way to adjust the step size intelligently:

\[
\begin{aligned}
m_k &= \beta_1m_{k-1} + (1-\beta_1)G_k, \quad m_0 = 0, \quad G_k = \nabla f(x_k), \quad \beta_1∈(0, \beta_2)\\
z_k &= \beta_2 z_{k-1} + (1-\beta_2)(G_k \odot G_k), \quad \beta_2∈(0, 1), \quad z_0=0\\
\hat{m}_k &= \frac{m_k}{1 - \beta_1^k} \quad(\text{exponentate at ktth iteration})\\
\hat{z}_k &= \max(\hat{z}_{k-1}, z_k), \quad \hat{z}_0 = 0 \\
\tilde{z}_k(i) &= \frac{1}{\sqrt{\hat{z}_k(i)} + \epsilon}\\ 
\mathbf{x_{k+1}} &= \boxed{x_k - \eta(\tilde{z}_k \odot \hat{m}_k), \quad \eta > 0}
\end{aligned}
\]

\textbf{Implement AMSGRAD-ADAM}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# logistic gradient descent AMSGRAD{-}ADAM}
\NormalTok{log\_adam }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(X, y, }\AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{, }\AttributeTok{max\_iter =} \DecValTok{10000}\NormalTok{, }\AttributeTok{eta =} \DecValTok{1}\NormalTok{, }\AttributeTok{epsilon =} \FloatTok{1e{-}8}\NormalTok{, }\AttributeTok{b\_1 =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{b\_2 =} \FloatTok{0.999}\NormalTok{) \{}
  \CommentTok{\# Initialize}
\NormalTok{  n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(X)}
\NormalTok{  p }\OtherTok{\textless{}{-}} \FunctionTok{ncol}\NormalTok{(X)}
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(X)}
\NormalTok{  y }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(y)}
\NormalTok{  beta }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, p))}
\NormalTok{  obj\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(max\_iter)}
\NormalTok{  eta\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(max\_iter)  }\CommentTok{\# To store eta values used each iteration}
\NormalTok{  beta\_values }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{() }\CommentTok{\# To store beta values used each iteration}
\NormalTok{  eta\_bt }\OtherTok{\textless{}{-}} \DecValTok{1}  \CommentTok{\# Initial step size for backtracking}
  
  \CommentTok{\# Objective function: negative log{-}likelihood}
  \CommentTok{\# input: Beta vector, x matrix, y matrix}
  \CommentTok{\# output: scalar objective func value}
  \CommentTok{\# comments: We want to minimize this function for logit regression}
\NormalTok{  obj\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, x, y) \{}
\NormalTok{    m }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(x)}
\NormalTok{    z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{\%*\%}\NormalTok{ beta}
\NormalTok{    (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ m) }\SpecialCharTok{*}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{(}\FunctionTok{t}\NormalTok{(y) }\SpecialCharTok{\%*\%}\NormalTok{ z) }\SpecialCharTok{+} \FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(z))))}
\NormalTok{  \}}
  
  \CommentTok{\# Gradient function}
  \CommentTok{\# input: Beta vector, x matrix, y matrix}
  \CommentTok{\# output: gradient vector in the dimension of nrow(Beta) x 1}
  \CommentTok{\# comments: We use this for gradient descent}
\NormalTok{  gradient }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, x, y) \{}
\NormalTok{    m }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(x)                       }\CommentTok{\# define m}
\NormalTok{    sig }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(z) }\DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{z))  }\CommentTok{\# sigmoid function}
\NormalTok{    (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ m) }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{t}\NormalTok{(x) }\SpecialCharTok{\%*\%}\NormalTok{ (}\FunctionTok{sig}\NormalTok{(x }\SpecialCharTok{\%*\%}\NormalTok{ beta) }\SpecialCharTok{{-}}\NormalTok{ y))}
\NormalTok{  \}}

  \CommentTok{\# Algorithm:}
  \ControlFlowTok{for}\NormalTok{ (iter }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{max\_iter) \{}
\NormalTok{    grad }\OtherTok{\textless{}{-}} \FunctionTok{gradient}\NormalTok{(beta, x, y)}
    
    \CommentTok{\#cat("iter ", iter, "\textbackslash{}n")}
    
    \CommentTok{\# ADAM step}
    \ControlFlowTok{if}\NormalTok{ (iter }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) \{}
\NormalTok{      m\_k }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ b\_1) }\SpecialCharTok{*}\NormalTok{ grad}
\NormalTok{      z\_k }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ b\_2) }\SpecialCharTok{*}\NormalTok{ grad}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{      m\_hat\_k }\OtherTok{\textless{}{-}}\NormalTok{ m\_k }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ b\_1}\SpecialCharTok{\^{}}\NormalTok{iter)}
\NormalTok{      z\_hat\_k }\OtherTok{\textless{}{-}} \FunctionTok{pmax}\NormalTok{(}\DecValTok{0}\NormalTok{, z\_k)}
\NormalTok{      z\_tild\_k }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\FunctionTok{sqrt}\NormalTok{(z\_hat\_k) }\SpecialCharTok{+}\NormalTok{ epsilon)}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{      m\_k }\OtherTok{\textless{}{-}}\NormalTok{ b\_1 }\SpecialCharTok{*}\NormalTok{ m\_k\_prev }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ b\_1) }\SpecialCharTok{*}\NormalTok{ grad}
\NormalTok{      z\_k }\OtherTok{\textless{}{-}}\NormalTok{ b\_2 }\SpecialCharTok{*}\NormalTok{ z\_k\_prev }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ b\_2) }\SpecialCharTok{*}\NormalTok{ grad}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{      m\_hat\_k }\OtherTok{\textless{}{-}}\NormalTok{ m\_k }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ b\_1}\SpecialCharTok{\^{}}\NormalTok{iter)}
\NormalTok{      z\_hat\_k }\OtherTok{\textless{}{-}} \FunctionTok{pmax}\NormalTok{(z\_hat\_k\_prev, z\_k)}
\NormalTok{      z\_tild\_k }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\FunctionTok{sqrt}\NormalTok{(z\_hat\_k) }\SpecialCharTok{+}\NormalTok{ epsilon)}
\NormalTok{    \}}
    
\NormalTok{    beta\_new }\OtherTok{\textless{}{-}}\NormalTok{ beta }\SpecialCharTok{{-}}\NormalTok{ eta }\SpecialCharTok{*}\NormalTok{ (z\_tild\_k }\SpecialCharTok{*}\NormalTok{ m\_hat\_k)}

    \CommentTok{\# current\_obj \textless{}{-} obj\_function(beta, x, y)}
    \CommentTok{\# grad\_norm\_sq \textless{}{-} sum(grad\^{}2)}
    \CommentTok{\# }
    \CommentTok{\# if(iter == 1) \{}
    \CommentTok{\#   eta\_bt \textless{}{-} 1}
    \CommentTok{\#   y\_k \textless{}{-} beta}
    \CommentTok{\# \} else \{}
    \CommentTok{\#   beta\_prev \textless{}{-} beta\_values[[iter {-} 1]]}
    \CommentTok{\#   xi \textless{}{-} (iter + 1) / (iter + 2)}
    \CommentTok{\#   y\_k \textless{}{-} beta + xi * (beta {-} beta\_prev)}
    \CommentTok{\# \}}
    \CommentTok{\# }
    \CommentTok{\# beta\_new \textless{}{-} y\_k {-} eta\_bt * grad}
    \CommentTok{\# }
    \CommentTok{\# while (obj\_function(beta\_new, x, y) \textgreater{} current\_obj {-} epsilon * eta\_bt * grad\_norm\_sq) \{}
    \CommentTok{\#   eta\_bt \textless{}{-} tau * eta\_bt}
    \CommentTok{\#   beta\_new \textless{}{-} beta {-} eta\_bt * grad}
    \CommentTok{\# \}}
    
    \CommentTok{\# save values to the matrix}
\NormalTok{    eta\_values[iter] }\OtherTok{\textless{}{-}}\NormalTok{ eta\_bt}
\NormalTok{    obj\_values[iter] }\OtherTok{\textless{}{-}} \FunctionTok{obj\_function}\NormalTok{(beta\_new, x, y)}
\NormalTok{    beta\_values[[iter]] }\OtherTok{\textless{}{-}}\NormalTok{ beta\_new}
    
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{((beta\_new }\SpecialCharTok{{-}}\NormalTok{ beta)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\textless{}}\NormalTok{ tol) \{}
      \CommentTok{\# set the vector ranges and break}
\NormalTok{      beta }\OtherTok{\textless{}{-}}\NormalTok{ beta\_new}
\NormalTok{      obj\_values }\OtherTok{\textless{}{-}}\NormalTok{ obj\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{iter]}
\NormalTok{      eta\_values }\OtherTok{\textless{}{-}}\NormalTok{ eta\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{iter]}
\NormalTok{      beta\_values }\OtherTok{\textless{}{-}}\NormalTok{ beta\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{iter]}
      \ControlFlowTok{break}
\NormalTok{    \}}
    
\NormalTok{    beta }\OtherTok{\textless{}{-}}\NormalTok{ beta\_new}
\NormalTok{    z\_k\_prev }\OtherTok{\textless{}{-}}\NormalTok{ z\_k}
\NormalTok{    m\_k\_prev }\OtherTok{\textless{}{-}}\NormalTok{ m\_k}
\NormalTok{    z\_hat\_k\_prev }\OtherTok{\textless{}{-}}\NormalTok{ z\_hat\_k}
\NormalTok{  \}}
  
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{beta =}\NormalTok{ beta, }\AttributeTok{obj\_values =}\NormalTok{ obj\_values, }\AttributeTok{eta\_values =}\NormalTok{ eta\_values, }\AttributeTok{beta\_values =}\NormalTok{ beta\_values))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\textbf{TESTING: AMSGRAD-ADAM}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log\_reg\_adam }\OtherTok{\textless{}{-}} \FunctionTok{log\_adam}\NormalTok{(X, y, }\AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{, }\AttributeTok{max\_iter =} \DecValTok{10000}\NormalTok{, }\AttributeTok{eta =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{epsilon =} \FloatTok{1e{-}8}\NormalTok{, }\AttributeTok{b\_1 =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{b\_2 =} \FloatTok{0.999}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{PRINTING OUTPUT}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"betas }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
betas 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(log\_reg\_adam}\SpecialCharTok{$}\NormalTok{beta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                 y
X1   -0.1418454849
X2   -0.0601519765
X3    0.1588541105
X4    0.1328432036
X5   -0.0480698927
X6    0.0992738274
X7    0.1190031009
X8    0.1165771060
X9    0.0121186568
X10   0.0002692862
X11   0.0440526339
X12  -0.1794460643
X13  -0.0107336589
X14  -0.1230683354
X15   0.0724951933
X16   0.0572069676
X17   0.1299658957
X18   0.1249286333
X19  -0.0018093352
X20   0.1249220832
X21  -0.0107937892
X22  -0.1432400037
X23  -0.1095158039
X24   0.0576442709
X25  -0.1190438042
X26   0.0164821239
X27  -0.0977823806
X28   0.1544852906
X29  -0.0276528153
X30   0.0164067155
X31  -0.0589237822
X32   0.0205429977
X33   0.1352414127
X34  -0.0301992700
X35  -0.0097282034
X36   0.0631470517
X37   0.1972976603
X38   0.0932627995
X39   0.1242672243
X40   0.1466160415
X41   0.1113132623
X42  -0.1226767443
X43  -0.0375124313
X44  -0.0155639906
X45  -0.0103119024
X46  -0.1807752324
X47   0.0122969426
X48   0.0309593047
X49   0.0257831520
X50   0.1231058283
X51  -0.0237022950
X52  -0.0136668437
X53   0.0802692451
X54   0.1696197561
X55   0.1711788101
X56  -0.0447894933
X57  -0.0407522017
X58  -0.0768854044
X59   0.0786508030
X60  -0.1192481480
X61  -0.0080695175
X62   0.0701660302
X63   0.0295340139
X64  -0.1090452901
X65   0.0634084702
X66  -0.1451071892
X67   0.1404697803
X68   0.0649124105
X69  -0.1596141322
X70   0.1128300914
X71   0.1889244806
X72   0.0920823098
X73  -0.0647856673
X74  -0.0684613155
X75   0.2307043702
X76  -0.1312463971
X77   0.0301756741
X78  -0.0742119633
X79   0.0695864468
X80  -0.0273958006
X81   0.0183803428
X82   0.0555629189
X83  -0.0196148832
X84  -0.0119195971
X85   0.0981231232
X86   0.1724823705
X87   0.0832704525
X88  -0.0070402199
X89   0.0720618891
X90   0.0779178976
X91   0.0026821896
X92  -0.1224121573
X93   0.0073619631
X94  -0.0996671370
X95  -0.0486034289
X96   0.0338488127
X97   0.1497352579
X98   0.1702661451
X99   0.0197954507
X100  0.0070025016
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"The function converged after"}\NormalTok{, }\FunctionTok{length}\NormalTok{(log\_reg\_adam}\SpecialCharTok{$}\NormalTok{obj\_values), }\StringTok{" iterations }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The function converged after 279  iterations 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"Eta Vals: }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Eta Vals: 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(log\_reg\_adam}\SpecialCharTok{$}\NormalTok{eta\_values[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[39] 1 1 1 1 1 1 1 1 1 1 1 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"Objective Function vals }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Objective Function vals 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(log\_reg\_adam}\SpecialCharTok{$}\NormalTok{obj\_values[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1]        Inf        Inf        Inf        Inf 22.2832809 14.8373097
 [7]  7.1284249  2.0996220  6.1137497  1.3895229  2.9819074  3.8056161
[13]  3.8809432  3.3404494  2.3347514  1.1762725  2.9227801  1.5778861
[19]  1.3114477  2.0080097  2.3413146  2.2664773  1.8437349  1.2165596
[25]  1.0948241  1.9858414  0.9086495  1.1820604  1.4516938  1.4476730
[31]  1.1868100  0.8328216  1.1328824  0.9287048  0.7908114  0.9833457
[37]  0.9977538  0.8109798  0.6450300  0.9912377  0.6135863  0.7809745
[43]  0.8118218  0.6527392  0.5829003  0.7306665  0.6084322  0.7789611
[49]  0.7352627  0.5369212
\end{verbatim}

\subsubsection{(4) Stochastic gradient descent with a fixed schedule of
decreasing step
sizes}\label{stochastic-gradient-descent-with-a-fixed-schedule-of-decreasing-step-sizes}

Stochastic gradient descent happens is an implementation of gradient
descent that adds randomness by calculating a gradient as a subset of
the data points in order to try to get the algorithm to converge

Algorithm (SGD)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select the cardinality s of index set \(I_k\)
\item
  Select \(x_0∈\mathbb{R}^n\)
\item
  While stopping criterion \textgreater{} tol, do:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \(x_{k+1} = x_k - \eta_{k}\nabla f_{I_k}(x_k)\)
\item
  Calculate the value of the stopping criterion
\end{itemize}

Note that:

\[
f_{I_k}(x_k) = \frac{1}{s} \sum_{i∈I_k} f_i(x_k), \quad \nabla{[f_{I_k}(x_k)]} = \frac{1}{s} \sum_{i∈I_k} \nabla f_i(x_k)
\]

\textbf{Implement SGD}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stochastic gradient descent with fixed schedule of decreasing step size}
\NormalTok{log\_sgd }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(X, y, }\AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{, }\AttributeTok{max\_iter =} \DecValTok{10000}\NormalTok{, }\AttributeTok{s =} \DecValTok{32}\NormalTok{, }\AttributeTok{eta =} \DecValTok{1}\NormalTok{) \{}
  \CommentTok{\# Initialize}
\NormalTok{  n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(X)}
\NormalTok{  p }\OtherTok{\textless{}{-}} \FunctionTok{ncol}\NormalTok{(X)}
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(X)}
\NormalTok{  y }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(y)}
\NormalTok{  beta }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, p))}
\NormalTok{  obj\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(max\_iter)}
\NormalTok{  eta\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(max\_iter)  }\CommentTok{\# To store eta values used each iteration}
\NormalTok{  beta\_values }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{() }\CommentTok{\# To store beta values used each iteration}
  
  \CommentTok{\# Objective function: negative log{-}likelihood}
  \CommentTok{\# input: Beta vector, x matrix, y matrix}
  \CommentTok{\# output: scalar objective func value}
  \CommentTok{\# comments: We want to minimize this function for logit regression}
\NormalTok{  obj\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, x, y) \{}
\NormalTok{    m }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(x)}
\NormalTok{    z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{\%*\%}\NormalTok{ beta}
\NormalTok{    (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ m) }\SpecialCharTok{*}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{(}\FunctionTok{t}\NormalTok{(y) }\SpecialCharTok{\%*\%}\NormalTok{ z) }\SpecialCharTok{+} \FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(z))))}
\NormalTok{  \}}
  
\NormalTok{  obj\_sum }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, x, y, subset) \{}
\NormalTok{    x\_sub }\OtherTok{\textless{}{-}}\NormalTok{ x[subset, , drop }\OtherTok{=} \ConstantTok{FALSE}\NormalTok{]   }\CommentTok{\# subset of x}
\NormalTok{    y\_sub }\OtherTok{\textless{}{-}}\NormalTok{ y[subset, , drop }\OtherTok{=} \ConstantTok{FALSE}\NormalTok{]   }\CommentTok{\# subset of y}
    \FunctionTok{obj\_function}\NormalTok{(beta, x\_sub, y\_sub)}
\NormalTok{  \}}
  
  \CommentTok{\# Gradient function}
  \CommentTok{\# input: Beta vector, x matrix, y matrix}
  \CommentTok{\# output: gradient vector in the dimension of nrow(Beta) x 1}
  \CommentTok{\# comments: We use this for gradient descent}
\NormalTok{  gradient }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, x, y) \{}
\NormalTok{    m }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(x)                       }\CommentTok{\# define m}
\NormalTok{    sig }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(z)\{ }
\NormalTok{      z }\OtherTok{\textless{}{-}} \FunctionTok{pmin}\NormalTok{(z, }\DecValTok{20}\NormalTok{)  }\CommentTok{\# Clip high values}
\NormalTok{      z }\OtherTok{\textless{}{-}} \FunctionTok{pmax}\NormalTok{(z, }\SpecialCharTok{{-}}\DecValTok{20}\NormalTok{) }\CommentTok{\# Clip l}
      \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{z))  }\CommentTok{\# sigmoid function}
\NormalTok{    \}}
\NormalTok{    (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ m) }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{t}\NormalTok{(x) }\SpecialCharTok{\%*\%}\NormalTok{ (}\FunctionTok{sig}\NormalTok{(x }\SpecialCharTok{\%*\%}\NormalTok{ beta) }\SpecialCharTok{{-}}\NormalTok{ y))}
\NormalTok{  \}}
  
\NormalTok{  grad\_sum }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, x, y, subset) \{}
\NormalTok{    x\_sub }\OtherTok{\textless{}{-}}\NormalTok{ x[subset, , drop }\OtherTok{=} \ConstantTok{FALSE}\NormalTok{]   }\CommentTok{\# subset of x}
\NormalTok{    y\_sub }\OtherTok{\textless{}{-}}\NormalTok{ y[subset, , drop }\OtherTok{=} \ConstantTok{FALSE}\NormalTok{]   }\CommentTok{\# subset of y}
    \FunctionTok{gradient}\NormalTok{(beta, x\_sub, y\_sub)}
\NormalTok{  \}}

  \CommentTok{\# Algorithm:}
  \ControlFlowTok{for}\NormalTok{ (iter }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{max\_iter) \{}

\NormalTok{    eta\_k }\OtherTok{=}\NormalTok{ eta }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FloatTok{0.001} \SpecialCharTok{*}\NormalTok{ iter)}
    
    \CommentTok{\# subset of data}
\NormalTok{    subset }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, s, }\AttributeTok{replace=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{    obj\_sub }\OtherTok{\textless{}{-}} \FunctionTok{obj\_sum}\NormalTok{(beta, x, y, subset)}
\NormalTok{    grad\_sub }\OtherTok{\textless{}{-}} \FunctionTok{grad\_sum}\NormalTok{(beta, x, y, subset)}
    
\NormalTok{    beta\_new }\OtherTok{\textless{}{-}}\NormalTok{ beta }\SpecialCharTok{{-}}\NormalTok{ eta\_k }\SpecialCharTok{*}\NormalTok{ grad\_sub}
    
    \CommentTok{\# save values to the matrix}
\NormalTok{    eta\_values[iter] }\OtherTok{\textless{}{-}}\NormalTok{ eta\_k}
\NormalTok{    obj\_values[iter] }\OtherTok{\textless{}{-}}\NormalTok{ obj\_sub}
\NormalTok{    beta\_values[[iter]] }\OtherTok{\textless{}{-}}\NormalTok{ beta\_new}
    
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{((beta\_new }\SpecialCharTok{{-}}\NormalTok{ beta)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\textless{}}\NormalTok{ tol) \{}
      \CommentTok{\# set the vector ranges and break}
\NormalTok{      beta }\OtherTok{\textless{}{-}}\NormalTok{ beta\_new}
\NormalTok{      obj\_values }\OtherTok{\textless{}{-}}\NormalTok{ obj\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{iter]}
\NormalTok{      eta\_values }\OtherTok{\textless{}{-}}\NormalTok{ eta\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{iter]}
\NormalTok{      beta\_values }\OtherTok{\textless{}{-}}\NormalTok{ beta\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{iter]}
      \ControlFlowTok{break}
\NormalTok{    \}}
    
\NormalTok{    beta }\OtherTok{\textless{}{-}}\NormalTok{ beta\_new}
    \ControlFlowTok{if}\NormalTok{ (iter }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{||}\NormalTok{ iter }\SpecialCharTok{\%\%} \DecValTok{1000} \SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\FunctionTok{cat}\NormalTok{(}\StringTok{"iter"}\NormalTok{, iter, }\StringTok{"eta:"}\NormalTok{, eta\_k, }\StringTok{"obj:"}\NormalTok{, obj\_sub, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{  \}}
  
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{beta =}\NormalTok{ beta, }\AttributeTok{obj\_values =}\NormalTok{ obj\_values, }\AttributeTok{eta\_values =}\NormalTok{ eta\_values, }\AttributeTok{beta\_values =}\NormalTok{ beta\_values))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\textbf{TESTING: SGD(No ADAM)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log\_reg\_sgd }\OtherTok{\textless{}{-}} \FunctionTok{log\_sgd}\NormalTok{(X, y, }\AttributeTok{tol =} \FloatTok{1e{-}4}\NormalTok{, }\AttributeTok{max\_iter =} \DecValTok{10000}\NormalTok{, }\AttributeTok{s =} \DecValTok{256}\NormalTok{, }\AttributeTok{eta =} \FloatTok{0.001}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
iter 1 eta: 0.000999001 obj: 0.6931472 
\end{verbatim}

\textbf{PRINTING OUTPUT}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"betas }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
betas 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(log\_reg\_sgd}\SpecialCharTok{$}\NormalTok{beta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              y
X1   0.01135434
X2   0.01553247
X3   0.02547594
X4   0.02478308
X5   0.01649211
X6   0.02267313
X7   0.02279703
X8   0.02286618
X9   0.01939711
X10  0.01766395
X11  0.02058742
X12  0.01023013
X13  0.01838424
X14  0.01132511
X15  0.02220197
X16  0.01983699
X17  0.02474106
X18  0.02455106
X19  0.01809302
X20  0.02237795
X21  0.01786076
X22  0.01215984
X23  0.01263780
X24  0.02156132
X25  0.01241035
X26  0.01898106
X27  0.01457513
X28  0.02594599
X29  0.01643030
X30  0.01906561
X31  0.01544249
X32  0.01858988
X33  0.02373124
X34  0.01690271
X35  0.01768783
X36  0.02072269
X37  0.02700170
X38  0.02219808
X39  0.02400643
X40  0.02591264
X41  0.02360806
X42  0.01282970
X43  0.01740925
X44  0.01747289
X45  0.01663346
X46  0.01006871
X47  0.01872684
X48  0.01928490
X49  0.01917166
X50  0.02379881
X51  0.01596886
X52  0.01697237
X53  0.02194963
X54  0.02668542
X55  0.02570057
X56  0.01660433
X57  0.01652876
X58  0.01447817
X59  0.02157667
X60  0.01216898
X61  0.01935009
X62  0.02070880
X63  0.01855471
X64  0.01285274
X65  0.02124817
X66  0.01023270
X67  0.02506424
X68  0.02185364
X69  0.01003059
X70  0.02329080
X71  0.02609856
X72  0.02219396
X73  0.01473030
X74  0.01507792
X75  0.02978834
X76  0.01263462
X77  0.01944849
X78  0.01418123
X79  0.02073226
X80  0.01686592
X81  0.01860687
X82  0.01930083
X83  0.01695688
X84  0.01896319
X85  0.02295664
X86  0.02579406
X87  0.02208678
X88  0.01823207
X89  0.02214396
X90  0.02211423
X91  0.01753706
X92  0.01355594
X93  0.01862842
X94  0.01360701
X95  0.01685557
X96  0.02016261
X97  0.02535342
X98  0.02580509
X99  0.01852538
X100 0.01809922
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"The function converged after"}\NormalTok{, }\FunctionTok{length}\NormalTok{(log\_reg\_sgd}\SpecialCharTok{$}\NormalTok{obj\_values), }\StringTok{" iterations }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The function converged after 774  iterations 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"Eta Vals: }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Eta Vals: 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(log\_reg\_sgd}\SpecialCharTok{$}\NormalTok{eta\_values[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 0.0009990010 0.0009980040 0.0009970090 0.0009960159 0.0009950249
 [6] 0.0009940358 0.0009930487 0.0009920635 0.0009910803 0.0009900990
[11] 0.0009891197 0.0009881423 0.0009871668 0.0009861933 0.0009852217
[16] 0.0009842520 0.0009832842 0.0009823183 0.0009813543 0.0009803922
[21] 0.0009794319 0.0009784736 0.0009775171 0.0009765625 0.0009756098
[26] 0.0009746589 0.0009737098 0.0009727626 0.0009718173 0.0009708738
[31] 0.0009699321 0.0009689922 0.0009680542 0.0009671180 0.0009661836
[36] 0.0009652510 0.0009643202 0.0009633911 0.0009624639 0.0009615385
[41] 0.0009606148 0.0009596929 0.0009587728 0.0009578544 0.0009569378
[46] 0.0009560229 0.0009551098 0.0009541985 0.0009532888 0.0009523810
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"Objective Function vals }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Objective Function vals 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(log\_reg\_sgd}\SpecialCharTok{$}\NormalTok{obj\_values[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 0.6931472 0.6899505 0.6883899 0.6831475 0.6787769 0.6762963 0.6759592
 [8] 0.6700340 0.6647162 0.6658464 0.6666402 0.6602883 0.6474681 0.6558720
[15] 0.6591510 0.6485301 0.6458368 0.6404269 0.6463510 0.6531543 0.6455916
[22] 0.6466601 0.6287876 0.6409876 0.6285182 0.6214071 0.6170738 0.6352378
[29] 0.6218606 0.6279371 0.6307798 0.6123240 0.6188174 0.6289519 0.6282660
[36] 0.6089209 0.6031430 0.6105668 0.5921086 0.6086099 0.5997608 0.6108823
[43] 0.6157268 0.6083084 0.6080561 0.6068213 0.5982542 0.5983205 0.5795877
[50] 0.6010424
\end{verbatim}

\subsubsection{(5) Stochastic gradient descent with AMSGrad-ADAM-W
momentum}\label{stochastic-gradient-descent-with-amsgrad-adam-w-momentum}

(no backtracking line search, since AMSGrad-ADAM adjusts step sizes per
parameter using momentum and adaptive scaling)

We can apply the AMSGrad-ADAM update to the stochastic gradient
algorithm shown previously, except multiplying (1 − ηλ) to \(x_k\):

\textbf{Implement SGD ADAM}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stochastic gradient descent with fixed schedule of decreasing step size}
\NormalTok{log\_sgd\_adam }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(X, y, }\AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{, }\AttributeTok{max\_iter =} \DecValTok{10000}\NormalTok{, }\AttributeTok{lambda =} \FloatTok{1e{-}4}\NormalTok{, }\AttributeTok{s =} \DecValTok{32}\NormalTok{, }\AttributeTok{eta =} \DecValTok{1}\NormalTok{, }\AttributeTok{epsilon =} \FloatTok{1e{-}8}\NormalTok{, }\AttributeTok{b\_1 =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{b\_2 =} \FloatTok{0.999}\NormalTok{) \{}
  \CommentTok{\# Initialize}
\NormalTok{  n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(X)}
\NormalTok{  p }\OtherTok{\textless{}{-}} \FunctionTok{ncol}\NormalTok{(X)}
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(X)}
\NormalTok{  y }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(y)}
\NormalTok{  beta }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, p))}
\NormalTok{  obj\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(max\_iter)}
\NormalTok{  eta\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(max\_iter)  }\CommentTok{\# To store eta values used each iteration}
\NormalTok{  beta\_values }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{() }\CommentTok{\# To store beta values used each iteration}
  
  \CommentTok{\# Objective function: negative log{-}likelihood}
  \CommentTok{\# input: Beta vector, x matrix, y matrix}
  \CommentTok{\# output: scalar objective func value}
  \CommentTok{\# comments: We want to minimize this function for logit regression}
\NormalTok{   obj\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, x, y) \{}
\NormalTok{    m }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(x)}
\NormalTok{    z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{\%*\%}\NormalTok{ beta}
\NormalTok{    (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ m) }\SpecialCharTok{*}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{(}\FunctionTok{t}\NormalTok{(y) }\SpecialCharTok{\%*\%}\NormalTok{ z) }\SpecialCharTok{+} \FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(z))))}
\NormalTok{  \}}
  
\NormalTok{  obj\_sum }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, x, y, subset) \{}
\NormalTok{    x\_sub }\OtherTok{\textless{}{-}}\NormalTok{ x[subset, , drop }\OtherTok{=} \ConstantTok{FALSE}\NormalTok{]   }\CommentTok{\# subset of x}
\NormalTok{    y\_sub }\OtherTok{\textless{}{-}}\NormalTok{ y[subset, , drop }\OtherTok{=} \ConstantTok{FALSE}\NormalTok{]   }\CommentTok{\# subset of y}
    \FunctionTok{obj\_function}\NormalTok{(beta, x\_sub, y\_sub)}
\NormalTok{  \}}
  
  \CommentTok{\# Gradient function}
  \CommentTok{\# input: Beta vector, x matrix, y matrix}
  \CommentTok{\# output: gradient vector in the dimension of nrow(Beta) x 1}
  \CommentTok{\# comments: We use this for gradient descent}
\NormalTok{  gradient }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, x, y) \{}
\NormalTok{    m }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(x)                       }\CommentTok{\# define m}
\NormalTok{    sig }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(z) }\DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{z))  }\CommentTok{\# sigmoid function}
\NormalTok{    (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ m) }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{t}\NormalTok{(x) }\SpecialCharTok{\%*\%}\NormalTok{ (}\FunctionTok{sig}\NormalTok{(x }\SpecialCharTok{\%*\%}\NormalTok{ beta) }\SpecialCharTok{{-}}\NormalTok{ y))}
\NormalTok{  \}}
  
\NormalTok{  grad\_sum }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, x, y, subset) \{}
\NormalTok{    x\_sub }\OtherTok{\textless{}{-}}\NormalTok{ x[subset, , drop }\OtherTok{=} \ConstantTok{FALSE}\NormalTok{]   }\CommentTok{\# subset of x}
\NormalTok{    y\_sub }\OtherTok{\textless{}{-}}\NormalTok{ y[subset, , drop }\OtherTok{=} \ConstantTok{FALSE}\NormalTok{]   }\CommentTok{\# subset of y}
    \FunctionTok{gradient}\NormalTok{(beta, x\_sub, y\_sub)}
\NormalTok{  \}}

  \CommentTok{\# Algorithm:}
  \ControlFlowTok{for}\NormalTok{ (iter }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{max\_iter) \{}
    
    \CommentTok{\# subset of data}
\NormalTok{    subset }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, s, }\AttributeTok{replace=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{    obj\_sub }\OtherTok{\textless{}{-}} \FunctionTok{obj\_sum}\NormalTok{(beta, x, y, subset)}
\NormalTok{    grad\_sub }\OtherTok{\textless{}{-}} \FunctionTok{grad\_sum}\NormalTok{(beta, x, y, subset)}
    
    \CommentTok{\# ADAM step}
    \ControlFlowTok{if}\NormalTok{ (iter }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) \{}
\NormalTok{      m\_k }\OtherTok{\textless{}{-}}\NormalTok{ grad\_sub}
\NormalTok{      z\_k }\OtherTok{\textless{}{-}}\NormalTok{ grad\_sub}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{      m\_hat\_k }\OtherTok{\textless{}{-}}\NormalTok{ m\_k }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ b\_1}\SpecialCharTok{\^{}}\NormalTok{iter)}
\NormalTok{      z\_hat\_k }\OtherTok{\textless{}{-}} \FunctionTok{pmax}\NormalTok{(}\DecValTok{0}\NormalTok{, z\_k)}
\NormalTok{      z\_tild\_k }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\FunctionTok{sqrt}\NormalTok{(z\_hat\_k) }\SpecialCharTok{+}\NormalTok{ epsilon)}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{      m\_k }\OtherTok{\textless{}{-}}\NormalTok{ b\_1 }\SpecialCharTok{*}\NormalTok{ m\_k\_prev }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ b\_1) }\SpecialCharTok{*}\NormalTok{ grad\_sub}
\NormalTok{      z\_k }\OtherTok{\textless{}{-}}\NormalTok{ b\_2 }\SpecialCharTok{*}\NormalTok{ z\_k\_prev }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ b\_2) }\SpecialCharTok{*}\NormalTok{ grad\_sub}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{      m\_hat\_k }\OtherTok{\textless{}{-}}\NormalTok{ m\_k }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ b\_1}\SpecialCharTok{\^{}}\NormalTok{iter)}
\NormalTok{      z\_hat\_k }\OtherTok{\textless{}{-}} \FunctionTok{pmax}\NormalTok{(z\_hat\_k\_prev, z\_k)}
\NormalTok{      z\_tild\_k }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\FunctionTok{sqrt}\NormalTok{(z\_hat\_k) }\SpecialCharTok{+}\NormalTok{ epsilon)}
\NormalTok{    \}}
    
\NormalTok{    beta\_new }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ eta }\SpecialCharTok{*}\NormalTok{ lambda) }\SpecialCharTok{*}\NormalTok{ beta }\SpecialCharTok{{-}}\NormalTok{ eta }\SpecialCharTok{*}\NormalTok{ (z\_tild\_k }\SpecialCharTok{*}\NormalTok{ m\_hat\_k)}
    
    \CommentTok{\# save values to the matrix}
\NormalTok{    eta\_values[iter] }\OtherTok{\textless{}{-}}\NormalTok{ eta}
\NormalTok{    obj\_values[iter] }\OtherTok{\textless{}{-}} \FunctionTok{obj\_function}\NormalTok{(beta\_new, x, y)}
\NormalTok{    beta\_values[[iter]] }\OtherTok{\textless{}{-}}\NormalTok{ beta\_new}
    
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{((beta\_new }\SpecialCharTok{{-}}\NormalTok{ beta)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\textless{}}\NormalTok{ tol) \{}
      \CommentTok{\# set the vector ranges and break}
\NormalTok{      beta }\OtherTok{\textless{}{-}}\NormalTok{ beta\_new}
\NormalTok{      obj\_values }\OtherTok{\textless{}{-}}\NormalTok{ obj\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{iter]}
\NormalTok{      eta\_values }\OtherTok{\textless{}{-}}\NormalTok{ eta\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{iter]}
\NormalTok{      beta\_values }\OtherTok{\textless{}{-}}\NormalTok{ beta\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{iter]}
      \ControlFlowTok{break}
\NormalTok{    \}}
    
\NormalTok{    beta }\OtherTok{\textless{}{-}}\NormalTok{ beta\_new}
\NormalTok{    z\_k\_prev }\OtherTok{\textless{}{-}}\NormalTok{ z\_k}
\NormalTok{    m\_k\_prev }\OtherTok{\textless{}{-}}\NormalTok{ m\_k}
\NormalTok{    z\_hat\_k\_prev }\OtherTok{\textless{}{-}}\NormalTok{ z\_hat\_k}
    \ControlFlowTok{if}\NormalTok{ (iter }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{|}\NormalTok{ iter }\SpecialCharTok{\%\%} \DecValTok{1000} \SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\FunctionTok{cat}\NormalTok{(}\StringTok{"iter"}\NormalTok{, iter, }\StringTok{"obj:"}\NormalTok{, obj\_sub, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{  \}}
  
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{beta =}\NormalTok{ beta, }\AttributeTok{obj\_values =}\NormalTok{ obj\_values, }\AttributeTok{eta\_values =}\NormalTok{ eta\_values, }\AttributeTok{beta\_values =}\NormalTok{ beta\_values))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\textbf{TESTING: SGD ADAM}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log\_reg\_sgd\_adam }\OtherTok{\textless{}{-}} \FunctionTok{log\_sgd\_adam}\NormalTok{(X, y, }\AttributeTok{tol =} \FloatTok{1e{-}2}\NormalTok{, }\AttributeTok{max\_iter =} \DecValTok{10000}\NormalTok{, }\AttributeTok{lambda =} \FloatTok{1e{-}4}\NormalTok{, }\AttributeTok{s =} \DecValTok{256}\NormalTok{, }\AttributeTok{eta =} \FloatTok{0.01}\NormalTok{, }\AttributeTok{epsilon =} \FloatTok{1e{-}8}\NormalTok{, }\AttributeTok{b\_1 =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{b\_2 =} \FloatTok{0.999}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
iter 1 obj: 0.6931472 
\end{verbatim}

\textbf{PRINTING OUTPUT: SGD ADAM}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"betas }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
betas 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(log\_reg\_sgd\_adam}\SpecialCharTok{$}\NormalTok{beta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                 y
X1   -0.0846350487
X2   -0.0124010853
X3    0.0555628769
X4    0.0742999680
X5   -0.1182542697
X6    0.0558628759
X7    0.0293496739
X8    0.0442484052
X9    0.0077119300
X10  -0.1268388865
X11   0.0614244487
X12  -0.0745620848
X13  -0.0235281602
X14  -0.1118354964
X15  -0.0022481179
X16   0.0362880146
X17   0.0276927003
X18   0.0557341451
X19   0.0739836170
X20  -0.0447301030
X21   0.0528074479
X22   0.0162599817
X23  -0.0162785676
X24  -0.0954133547
X25  -0.0373110853
X26   0.0209924010
X27  -0.3112053982
X28   0.1055346118
X29   0.0190964421
X30  -0.0111074973
X31  -0.0812639465
X32   0.0674209252
X33   0.0778018010
X34   0.0110840244
X35   0.0474483632
X36   0.0766899453
X37   0.0889207977
X38   0.0569115340
X39   0.0059694992
X40   0.1064577032
X41   0.0848035758
X42  -0.0424669410
X43   0.0257388739
X44  -0.0567801372
X45  -0.0124715608
X46  -0.0189873300
X47  -0.0364654703
X48   0.0326952708
X49   0.0636014370
X50   0.0771965519
X51  -0.0197872936
X52  -0.1040558501
X53   0.0970089486
X54   0.0694368456
X55   0.0137015556
X56  -0.0155630584
X57  -0.0480951127
X58  -0.0892051386
X59   0.0719298726
X60  -0.1548228608
X61   0.0594450153
X62   0.0561337620
X63   0.0139001100
X64  -0.0551283259
X65  -0.0031146087
X66  -0.0620627266
X67  -0.0134451155
X68   0.0659925789
X69  -0.0828161146
X70   0.0671992452
X71   0.0634629106
X72   0.0064634833
X73  -0.0005192438
X74   0.0131385659
X75   0.0810782316
X76  -0.0357127540
X77   0.0109596326
X78  -0.0167523194
X79   0.0785550751
X80   0.0392163700
X81  -0.0466329074
X82  -0.0254647907
X83   0.0246743763
X84   0.0541367949
X85   0.0715176982
X86   0.0414844249
X87   0.0262101744
X88   0.0024564074
X89   0.0440199118
X90   0.0390924919
X91  -0.0161686477
X92  -0.0050758005
X93  -0.0229872255
X94  -0.0205879448
X95   0.0117065290
X96   0.0255148540
X97   0.0730530351
X98   0.0605628370
X99  -0.0051534570
X100  0.1013927258
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"The function converged after"}\NormalTok{, }\FunctionTok{length}\NormalTok{(log\_reg\_sgd\_adam}\SpecialCharTok{$}\NormalTok{obj\_values), }\StringTok{" iterations }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The function converged after 56  iterations 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"Eta Vals: }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Eta Vals: 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(log\_reg\_sgd\_adam}\SpecialCharTok{$}\NormalTok{eta\_values[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
[16] 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
[31] 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
[46] 0.01 0.01 0.01 0.01 0.01
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"Objective Function vals }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Objective Function vals 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(log\_reg\_sgd\_adam}\SpecialCharTok{$}\NormalTok{obj\_values[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 1.1161821 1.5373292 1.7827117 1.9423441 2.0494091 2.1201949 2.1661810
 [8] 2.1935314 2.2075204 2.2104329 2.2073275 2.1962513 2.1773307 2.1519954
[15] 2.1225932 2.0865053 2.0473293 2.0041901 1.9596392 1.9137775 1.8661258
[22] 1.8168482 1.7661386 1.7150601 1.6605808 1.6051586 1.5493324 1.4939364
[29] 1.4369382 1.3794203 1.3210101 1.2625243 1.2055373 1.1491443 1.0926230
[36] 1.0387216 0.9847929 0.9325850 0.8810229 0.8313522 0.7844547 0.7385439
[43] 0.6958187 0.6554445 0.6203532 0.5901666 0.5655170 0.5468710 0.5350448
[50] 0.5304285
\end{verbatim}

\subsection{Part (a) Hyperparameter
Discussion}\label{part-a-hyperparameter-discussion}

Discuss how you selected the various hyperparameters for each of the
algorithms

For BLS, I selected tau and epsilon = 0.5, because they should be
between 0 and 1 and 0.5 is relatively standard in order for it to
converge. That is fairly standard for the Armijo condition.

For BLS with Nesterov, I kept the hyperparameters the same as BLS
because it was standard from before, and then decided to set tau = 0.8
to keep the step size bigger and with faster convergence. The
convergence was the same, likely the momentum not making a huge
difference with this particular objective function

For SGD, The decreasing step size implemented was eta\_k = eta / (1 +
0.001 * iter), ensuring that eta decreases with every iteration, as it
is also a common algorithm used in literature to decrease eta. The
multiplier 0.001 means that eta won't significantly drop after 1000
iterations, otherwise eta would get too small. I also set eta = 0.001
initially, otherwise it would not converge.

For AMSGRAD-ADAM, I selected Beta1 = 0.9 and Beta2 = 0.999, In order
that Beta1 and Beta2 to not be to o small, and it is also a common step
size eta = 1 that is used in ADAM. The step size allowed it to converge
aggressively with few iterations

For AMSGRAD-ADAM-W with SGD, I selected the same coefficients as
AMSGRAD, it's just that I selected lambda to be a very small value
\textasciitilde1e-4, I had to keep the step size and tolerance

\subsection{Part (b) Metrics}\label{part-b-metrics}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ data, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{())}
\NormalTok{coefs }\OtherTok{\textless{}{-}}\NormalTok{ g}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{2}\SpecialCharTok{:}\DecValTok{101}\NormalTok{]}
\FunctionTok{print}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{((log\_reg\_bls}\SpecialCharTok{$}\NormalTok{beta }\SpecialCharTok{{-}}\NormalTok{ coefs)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.003482767
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{((log\_reg\_bls\_n}\SpecialCharTok{$}\NormalTok{beta }\SpecialCharTok{{-}}\NormalTok{ coefs)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.00348093
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{((log\_reg\_adam}\SpecialCharTok{$}\NormalTok{beta }\SpecialCharTok{{-}}\NormalTok{ coefs)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.003482475
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{((log\_reg\_sgd}\SpecialCharTok{$}\NormalTok{beta }\SpecialCharTok{{-}}\NormalTok{ coefs)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.915068
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{((log\_reg\_sgd\_adam}\SpecialCharTok{$}\NormalTok{beta }\SpecialCharTok{{-}}\NormalTok{ coefs)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.7681599
\end{verbatim}

For the algorithm BLS, BLS\_N, AMS\_ADAM, SGD, SGD\_AMS\_ADAM\_W

The estimation errors were: {[}1{]} 0.003482767, {[}1{]} 0.00348093,
{[}1{]} 0.003482475, {[}1{]} 0.915068, {[}1{]} 0.6947875

The iterations took 1909, 1909, 275, 769, and 56 respectively

Formatted Table:

\begin{longtable}[]{@{}lrr@{}}
\toprule\noalign{}
Algorithm & Estimation Error & Iterations \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
BLS & 0.003482767 & 1909 \\
BLS\_N & 0.00348093 & 1909 \\
AMS\_ADAM & 0.003482475 & 275 \\
SGD & 0.915068 & 769 \\
SGD\_AMS\_ADAM\_W & 0.6947875 & 56 \\
\end{longtable}

I see that ADAM performed very well in reducing the iterations for
converging, and we can see that all the models perform relatively well
in terms of estimation error. I would definitely use ADAM if I was going
to perform gradient descent in the future. For stochastic gradient
descent, it is likely that the tolerance needs to increase, hence the
estimation error also needs to increase a lot. However, the benefit is
that the function converges in less iterations, the example being
stochastic gradient descent with AMSGRAD-ADAM-W




\end{document}
