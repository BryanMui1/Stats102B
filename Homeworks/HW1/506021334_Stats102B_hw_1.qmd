---
title: "HW 1"
author: "Bryan Mui - UID 506021334 - 14 April 2025"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
         \DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\{\}}
---

Loaded packages: ggplot2, tidyverse (include = false for this chunk)
```{r options}
#| include: false
options(max.print = 50)
library(ggplot2)
library(tidyverse)
```


# Problem 1 

## Part a 

Find the theoretical min for the function: 
$$
f(x) = x^4 + 2x^2 + 1
$$

Solution: find f'(x) and f''(x), set f'(x) to 0 and solve, and f''(x) needs to be > 0 to be a min

Step 1: find f'(x) and f''(x)
\begin{align}
  f(x) &= x^4 + 2x^2 + 1 \\ 
 f'(x) &= 4x^3 + 4x  \\
f''(x) &= 12x^2 + 4 \\
\end {align}

Step 2: set f'(x) to 0 and solve

\begin{align}
f'(x) &= 4x^3 + 4x  \\
    0 &= 4x^3 + 4x \\
    0 &= 4x(x^2 + 4)
\end{align}


We get $$x = 0$$ and $$0 = x^2 +4$$ which has no real solution

Step 3: check that f''(x) needs to be > 0 to be a min

Our critical point is x = 0,  

\begin{align}
f''(0)  &= 12(0)^2 + 4 \\
        &= 4
\end{align}

Since f'(x) = 0 at 0 and f''(x) > 0 at that point, **we have a min at x = 0, and plugging into f(0) we get the minimum point**
\begin{center} (0, 1) \end{center}  


## Part b 

### 0) 

Use the gradient descent algorithm with **constant step size** and with **back-tracking line search** to calculate $x_{min}$

**Constant step size descent is implemented as follows:**

1. Select a random starting point $x_0$    
2. While stopping criteria < tolerance, do:  

  * Select $η_k$(as a constant)  
  * Calculate $x_{(k+1)} = x_k - η_k * ∇(f(x_k))$  
  * Calculate the value of stopping criterion  
  
Stopping criteria: Stop if $∣|∇(f(x_k)||_2 ≤ ϵ$ 
  

```{r grad_descent_constant_step}
# Gradient descent algorithm that uses constant step to minimize an objective function

gradient_descent_constant_step <- function(tol = 1e-6, max_iter = 10000, step_size = 0.01) {
  # Step 1: Initialize and select a random stopping point
  # Initialize
  set.seed(777) # example seeding 
  last_iter <- 0 # the last iteration ran
  eta <- step_size # step size that is decided manually 
  max_iter <- max_iter # max iterations before terminating if mininum isn't found
  tolerance <- tol # tolerance for the stoppign criteria 
  obj_values <- numeric(max_iter) # Stores the value of f(x)
  eta_values <- numeric(max_iter)  # To store eta values used each iteration
  eta_values[1] <- step_size
  betas <- numeric(max_iter) # Stores the value of x guesses
  x0 <- runif(1, min=-10, max=10) # our first guess is somewhere between -10-10
  
  # Set the objective function to the function to be minimized 
  # Objective function: f(x)
  obj_function <- function(x) {
    return(x^4 + 2*(x^2) + 1) 
  }
  
  # Gradient function: d/dx of f(x)
  gradient <- function(x) {
    return(4*x^3 + 4*x)
  }
  
  # Append the first guess to the obj_values and betas vector
  betas[1] <- x0
  obj_values[1] <- obj_function(x0)
  
  # Step 2: While stopping criteria < tolerance, do:
  for (iter in 1:max_iter) { # the iteration goes n = 1, 2, 3, 4, but the arrays of our output starts at iter = 0 and guess x0
    # Select eta(step size), which is constant
    # There's nothing to do for this step
    
    # Calculate the next guess of x_k+1, calculate f(x_k+1), set eta(x_k+1)
    betas[iter + 1] <- betas[iter] - (eta * gradient(betas[iter]))
    obj_values[iter + 1] <- obj_function(betas[iter + 1])
    eta_values[iter + 1] <- eta
    
    # Calculate the value of the stopping criterion
    stop_criteria <- abs(gradient(betas[iter + 1]))
    
    # If stopping criteria less than tolerance, break
    if(is.na(stop_criteria) || stop_criteria <= tolerance) { 
      last_iter <- iter + 1
      break 
    }
    
    # if we never set last iter, then we hit the max number of iterations and need to set
    if(last_iter == 0) { last_iter <- max_iter }
    
    # end algorithm
  }
  
  return(list(betas = betas, obj_values = obj_values, eta_values = eta_values, last_iter = last_iter)) # in this case, beta(predictors) are the x values, obj_values are f(x), eta is the step size, last iter is the value in the vector of the final iteration before stopping
}
```

Running the gradient descent algorithm with fixed step size:

```{r minimizing_constant_step}
minimize_constant_step <- gradient_descent_constant_step(tol = 1e-6, max_iter = 10000, step_size = 0.03)
print(minimize_constant_step)

cat("The functions stopped after", minimize_constant_step$last_iter - 1, "iterations \n")
cat("The function's point of minimization is", "(", minimize_constant_step$betas[minimize_constant_step$last_iter], "," , minimize_constant_step$obj_values[minimize_constant_step$last_iter], ") \n")
```

**Backtracking Line Search is implemented as follows:**  

1. Select a random starting point $x_0$    
2. While stopping criteria < tolerance, do:  

  * Select $η_k$ using backtracking line search
  * Calculate $x_{(k+1)} = x_k - η_k * ∇(f(x_k))$  
  * Calculate the value of stopping criterion  
  
Backtracking Line Search:  

  * Set $η^0 > 0$(usually a large value), $ϵ ∈ (0,1)$ and $τ ∈ (0,1)$
  * Set $η_1 = η^0$ 
  * At iteration k, set $η_k <- η_{k-1}$
    1. Check whether the Armijo Condition holds: 
    $$
    h(η_k) ≤ h(0) + ϵη_kh'(0)
    $$  
      where $h(η_k) = f(x_k) − η_k ∇f(x_k)$
    2. 
      + If yes(condition holds), terminate and keep $η_k$
      + If no, set $η_k = τη_k$ and go to Step 1

Stopping criteria: Stop if $∣|∇(f(x_k)||_2 ≤ ϵ$ 

Other note: Since we need h'(0) for the Armijo condition calculation, that is given by:  
$$
h'(0) = -[∇f(x_k)]^\top ∇f(x_k)
$$
Since we are minimizing x, we have a one dimensional beta, we can simplify to  
$$
h'(0) = -||∇f(x_k)||^2
$$

To summarize, backtracking line search chooses the step size by ensuring the Armijo condition always holds. If the Armijo condition doesn't hold, we are probably overshooting, hence the step size gets updated iteratively 

```{r backtracking_line_search_grad_descent}
gradient_descent_backtracking <- function(tol = 1e-6, max_iter = 10000, epsilon = 0.5, tau = 0.5, init_step_size = 1) {
  # Step 1: Initialize and select a random stopping point
  # Initialize
  set.seed(777) # example seeding 
  last_iter <- 0 # the last iteration ran
  max_iter <- max_iter # max iterations before terminating if minimum isn't found
  tolerance <- tol # tolerance for the stopping criteria 
  epsilon <- epsilon # Epsilon used in the step size criteria calculation
  tau <- tau # tau used in the step size criteria calculation
  obj_values <- numeric(max_iter) # Stores the value of f(x)
  eta_values <- numeric(max_iter)  # To store eta values used each iteration
  eta_values[1] <- init_step_size
  betas <- numeric(max_iter) # Stores the value of x guesses
  x0 <- runif(1, min=-10, max=10) # our first guess is somewhere between -10 to 10
  eta <- init_step_size # our initial step size
  
  # Set the objective function to the function to be minimized 
  # Objective function: f(x)
  obj_function <- function(x) {
    return(x^4 + 2*(x^2) + 1) 
  }
  
  # Gradient function: d/dx of f(x)
  gradient <- function(x) {
    return(4*x^3 + 4*x)
  }
  
  # Armijo condition function
  # returns TRUE or FALSE whether the condition is satisfied or not
  armijo_stepsize <- function(beta, eta, grad, f, epsilon, tau, max_iter) {
    subiter <- 1 # set a hard limit of iterations
    # calc armijo
    beta_new <- beta - (eta)*grad(beta)
    armijo <- f(beta_new) > (f(beta) - epsilon*eta*sum(grad(beta)^2))
    while (armijo && (iter <= max_iter)) {
      #update eta
      eta <- tau * eta
      
      #recalculate armijo
      beta_new <- beta - (eta)*grad(beta)
      armijo <- f(beta_new) > (f(beta) - epsilon*eta*sum(grad(beta)^2))
      
      subiter <- subiter + 1
    }
    return(eta)
  }
  
  # Append the first guess to the obj_values and betas vector
  betas[1] <- x0
  obj_values[1] <- obj_function(x0)
  
  # Step 2: While stopping criteria < tolerance, do:
  for (iter in 1:max_iter) { # the iteration goes n = 1, 2, 3, 4, but the arrays of our output starts at iter = 0 and guess x0
    beta <- betas[iter]
    
    # use BLS to calculate eta
    eta <- armijo_stepsize(beta = beta, eta = eta, grad = gradient, f = obj_function, epsilon = epsilon, tau = tau, max_iter = max_iter)
    eta_values[iter + 1] <- eta
    
    # Calculate the next guess of x_k+1
    beta_new <- beta - (eta * gradient(beta))
    betas[iter + 1] <- beta_new
    
    # calculate f(x_k+1), to keep track obj values
    obj_values[iter + 1] <- obj_function(beta_new)
    
    # Calculate the value of the stopping criterion
    stop_criteria <- abs(gradient(beta_new))
    
    # If stopping criteria less than tolerance, break
    if(is.na(stop_criteria) || stop_criteria <= tolerance) { 
      last_iter <- iter + 1
      break 
    }
    
    # if we never set last iter, then we hit the max number of iterations and need to set
    if(last_iter == 0) { last_iter <- max_iter }
    
    # end algorithm
  }
  
  return(list(betas = betas, obj_values = obj_values, eta_values = eta_values, last_iter = last_iter)) # in this case, beta(predictors) are the x values, obj_values are f(x), eta is the step size, last iter is the value in the vector of the final iteration before stopping
}
```

Running the gradient descent algorithm with backtracking:

```{r minimizing_backtracking}
minimize_backtrack <- gradient_descent_backtracking(tol = 1e-6, max_iter = 10000, epsilon = 0.5, tau = 0.8, init_step_size = 1)
print(minimize_backtrack)

cat("The functions stopped after", minimize_backtrack$last_iter - 1, "iterations \n")
cat("The function's point of minimization is", "(", minimize_backtrack$betas[minimize_backtrack$last_iter], "," , minimize_backtrack$obj_values[minimize_backtrack$last_iter], ") \n")
```

### 1) For the constant step size version of gradient descent, discuss how you selected the step size used in your code

Theoretical Analysis proves that for functions with a unique global minimum, the step size should be within 0 to 1/L to converge to the unique global minimum, where L is the Lipchitz constant, given by:  

$$
||∇f(x) - ∇f(y)||_2 \le L||x - y||_2
$$
Since this cannot be calculated in practice, usually a small step size of 0.01 is what to begin with. From there, manually fine tuning to try 0.02 and 0.03 is a good idea to see if there's any better iterations. Starting from a big step size is usually unsafe do the algorithm overshooting and diverging instead of converging. Ultimately, the step size of 0.02 seemed best to reduce the number of iterations.

### 2) For both versions of the gradient descent algorithm, plot the value of $f(x_k)$ as a function of k the number of iterations

```{r}
# constant step size
iterations <- 1:minimize_constant_step$last_iter
obj_values <- (minimize_constant_step$obj_values)[iterations]
f_k_constant <- cbind(obj_values, iterations)

iterations <- 1:minimize_backtrack$last_iter
obj_values <- (minimize_backtrack$obj_values)[iterations]
f_k_backtrack <- cbind(obj_values, iterations)

ggplot(f_k_constant, aes(x=iterations, y=obj_values)) + 
  geom_point() + 
  geom_line() + 
  ggtitle("Gradient Descent Convergence, Constant Step Size") + 
  xlab("Iteration") + ylab("Objective Function Value")
ggplot(f_k_backtrack, aes(x=iterations, y=obj_values)) + 
  geom_point() + 
  geom_line() + 
  ggtitle("Gradient Descent Convergence, Backtracking Line Search") + 
  xlab("Iteration") + ylab("Objective Function Value")
```


### 3) For the the gradient descent method with backtracking line search, plot the step size $η_k$ selected at step k as a function of k. Comment on the result

```{r}
iterations <- 1:minimize_backtrack$last_iter
eta_values <- minimize_backtrack$eta_values[iterations]
eta_backtrack <- cbind(eta_values, iterations)

ggplot(eta_backtrack, aes(x=iterations, y=eta_values)) + 
  geom_point(color = "blue") + 
  geom_line(color = "blue") + 
  ggtitle("Gradient Descent Eta Values, BLS") + 
  xlab("Iteration") + ylab("Eta Value")
```

We can see that the step size was initially 0.02, but at the very first few iterations the Armijo condition immediately reduced the step size to a very small number < 0.005 in order to prevent overshooting. This condition held for the rest of the iterations until the algorithm converged eventually, after around 443 iterations. Compared to the constant step size gradient descent, the step size was much smaller for all the iterations, meaning that it converged with > 300 more iterations than the constant step size gradient descent. Although using a large step size like 0.02 would be much faster, it seems like the step sizes were chosen to be a safer bound so that the algorithm would not overshoot

# Problem 2

**To understand the sensitivity of the gradient descent algorithm and its variants to the “shape” of the function, the two data sets provided (dataset1.csv, dataset2.csv) will be used**

They contain 100 observations for a response $y$ and 20 predictors $x_j, j = 1, · · · , 20$

## Part a 

Using the gradient descent code provided (both in R and Python) obtain the estimates of the regression coefficient, using both a constant step size and backtracking line search.

Read in the data and the gradient descent function:

```{r}
dataset1 <- read_csv("dataset1.csv")
dataset2 <- read_csv("dataset2.csv")

# gradient descent given in class
gradient_descent_class <- function(X, y, eta = NULL, tol = 1e-6, max_iter = 10000, backtracking = TRUE, epsilon = 0.5, tau = 0.8) {
  # Initialize
  n <- nrow(X)
  p <- ncol(X)
  beta <- rep(0, p)
  obj_values <- numeric(max_iter)
  eta_values <- numeric(max_iter)  # To store eta values used each iteration
  eta_bt <- 1  # Initial step size for backtracking
  
  # Objective function: Mean Squared Error (MSE)
  obj_function <- function(beta) {
    sum((X %*% beta - y)^2) / (2 * n)
  }
  
  # Gradient function
  gradient <- function(beta) {
    t(X) %*% (X %*% beta - y) / n
  }
  
  for (iter in 1:max_iter) {
    grad <- gradient(beta)
    
    if (backtracking) {
      if (iter == 1) eta_bt <- 1  # Reset only in the first iteration
      beta_new <- beta - eta_bt * grad
      
      while (obj_function(beta_new) > obj_function(beta) - epsilon * eta_bt * sum(grad^2)) {
        eta_bt <- tau * eta_bt
        beta_new <- beta - eta_bt * grad
      }
      eta_used <- eta_bt
    } else {
      if (is.null(eta)) stop("When backtracking is FALSE, a fixed eta must be provided.")
      beta_new <- beta - eta * grad
      eta_used <- eta
    }
    
    eta_values[iter] <- eta_used
    
    obj_values[iter] <- obj_function(beta_new)
    
    if (sqrt(sum((beta_new - beta)^2)) < tol) {
      obj_values <- obj_values[1:iter]
      eta_values <- eta_values[1:iter]
      break
    }
    
    beta <- beta_new
  }
  
  return(list(beta = beta, obj_values = obj_values, eta_values = eta_values))
}
```


```{r regression_constant_step}
#| fig-width: 10
X_dataset_1 <- dataset1 %>%
  select(-Y) %>%
  as.matrix()
y_dataset_1 <- dataset1 %>%
  select(Y) %>%
  as.matrix()
X_dataset_2 <- dataset2 %>%
  select(-Y) %>%
  as.matrix()
y_dataset_2 <- dataset2 %>%
  select(Y) %>%
  as.matrix()

reg_const_dataset_1 <- gradient_descent_class(X_dataset_1, y_dataset_1, backtracking = FALSE, eta = 5)

cat("Constant Step Size: dataset1 \n")
print("Beta Values:")
print(reg_const_dataset_1$beta)
print("Obj Function Values:")
print(reg_const_dataset_1$obj_values)
print("Eta Values:")
print(reg_const_dataset_1$eta_values)
cat("The functions stopped after", max(which(!is.na(reg_const_dataset_1$eta_values))), "iterations \n \n")

reg_const_dataset_2 <- gradient_descent_class(X_dataset_2, y_dataset_2, backtracking = FALSE, eta = 0.02)

cat("Constant Step Size: dataset2 \n")
print("Beta Values:")
print(reg_const_dataset_2$beta)
print("Obj Function Values:")
print(reg_const_dataset_2$obj_values)
print("Eta Values:")
print(reg_const_dataset_2$eta_values)
cat("The functions stopped after", max(which(!is.na(reg_const_dataset_2$eta_values))), "iterations \n \n")
```


```{r regression_bls}
#| fig-width: 10
reg_bls_data1 <- gradient_descent_class(X_dataset_1, y_dataset_1, eta = NULL, tol = 1e-6, max_iter = 10000, backtracking = TRUE, epsilon = 0.5, tau = 0.8)

cat("BLS: dataset1 \n")
print("Beta Values:")
print(reg_bls_data1$beta)
print("Obj Function Values:")
print(reg_bls_data1$obj_values)
print("Eta Values:")
print(reg_bls_data1$eta_values)
cat("The functions stopped after", max(which(!is.na(reg_bls_data1$eta_values))), "iterations \n \n")

reg_bls_data2 <- gradient_descent_class(X_dataset_2, y_dataset_2, eta = NULL, tol = 1e-6, max_iter = 10000, backtracking = TRUE, epsilon = 0.5, tau = 0.8)

cat("BLS: dataset2 \n")
print("Beta Values:")
print(reg_bls_data2$beta)
print("Obj Function Values:")
print(reg_bls_data2$obj_values)
print("Eta Values:")
print(reg_bls_data2$eta_values)
cat("The functions stopped after", max(which(!is.na(reg_bls_data2$eta_values))), "iterations \n \n")

```


### 1) Discuss how you selected the constant step size. Also, discuss which convergence criterion you used and the tolerance parameter used 

Because we cannot find the Lipchitz constant, constant step size is tuned manually. For data set 1, the step size initially was set small = 0.01, but it did not converge, so I increased the step size manually until it converged at step size = 1. From there, I pushed the step size until the function would not converge anymore. The max step size I could use without 5. For data set 2, I started with 0.01 step size, and eventually a step size of 0.02 was chosen in order to make the algorithm converge in 8159 iterations. For the tolerance parameter, a tolerance of $1e^-6$ was used, which is a standard tolerance parameter and is very close approximation to the real solution. For the convergence criterion, the stop criteria given in the code was to terminate when the difference of beta between iterations is less than the tolerance, indicating that the function has converged. 

### 2) Compare the results with those obtained from the lm command in R or from the class LinearRegression from the sklearn.linear model in Python. 

Specifically, calculate $∥ \hat{β_{GD}} − \hatβ∥_2$, where $\hat{β_{GD}}$ is the estimate of the regression coefficient obtained from the gradient descent algorithm (both with constant step size and backtracking line search) and $\hatβ$ obtained from the least squares solution implemented in R or Python

Use R's regression class:

```{r}
m1 <- lm(data = dataset1, Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + 
           X11 + X12 + X13 + X14 + X15 + X16 + X17 + X18 + X19 + X20)
summary(m1)

m2 <- lm(data = dataset2, Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + 
           X11 + X12 + X13 + X14 + X15 + X16 + X17 + X18 + X19 + X20)
summary(m2)
```

Plotting a table comparing $\beta$ to $\beta_{GD}$: 

```{r calc-beta-diff}
# transform dataset 1 coefs
beta_gd_d1 <- reg_const_dataset_1$beta
beta_1 <- m1$coefficients

names <- attributes(beta_gd_d1)$dimnames[[1]]
b_gd_1 <- as.vector(beta_gd_d1)
lm_1 <- as.vector(beta_1[-1])

d1 <- data.frame(Coef = names, Beta_GD = b_gd_1, Beta = lm_1)

# transform dataset 2 coefs
beta_gd_d2 <- reg_const_dataset_2$beta
beta_2 <- m2$coefficients

names <- attributes(beta_gd_d2)$dimnames[[1]]
b_gd_2 <- as.vector(beta_gd_d2)
lm_2 <- as.vector(beta_2[-1])

d2 <- data.frame(Coef = names, Beta_GD = b_gd_2, Beta = lm_2)

# show the difference between Beta_GD and Beta
d1 <- d1 %>%
  mutate(`Beta_GD - Beta` = Beta_GD - Beta)

d2 <- d2 %>%
  mutate(`Beta_GD - Beta` = Beta_GD - Beta)

print(d1)

print(d2)
```


Calculating the norm of difference in Betas:

The formula is given as:

$$
||\hat{\beta}_{GD} - \hat\beta||_2 = \sqrt{ \sum_{i=1}^{20} (\hat{\beta}_{GD, i} - \hat\beta_i)^2}
$$
Calculating the norm of the beta difference: 

```{r calc-norm}
d1_norm <- sqrt(sum(d1$`Beta_GD - Beta`^2))
d2_norm <- sqrt(sum(d2$`Beta_GD - Beta`^2))
cat("Dataset 1 Norm:", d1_norm, "\n")
cat("Dataset 2 Norm:", d2_norm, "\n")
```


### 3) Plot the value of the objective function as a function of the number of iterations required

```{r 2-a-3}
#| fig-width: 10
par(mfrow=c(2,2))

plot(reg_const_dataset_1$obj_values, type = "o", col = "blue", pch = 16, cex = 0.5,
     xlab = "Iteration", ylab = "Objective Function Value",
     main = "Gradient Descent Convergence, Const Step, dataset1")
plot(reg_const_dataset_2$obj_values, type = "o", col = "blue", pch = 16, cex = 0.5,
     xlab = "Iteration", ylab = "Objective Function Value",
     main = "Gradient Descent Convergence, Const Step, dataset2")
plot(reg_bls_data1$obj_values, type = "o", col = "blue", pch = 16, cex = 0.5,
     xlab = "Iteration", ylab = "Objective Function Value",
     main = "Gradient Descent Convergence, BLS, dataset1")
plot(reg_bls_data2$obj_values, type = "o", col = "blue", pch = 16, cex = 0.5,
     xlab = "Iteration", ylab = "Objective Function Value, dataset2",
     main = "Gradient Descent Convergence, BLS, dataset2")
```
## Part b

**Implement the Polyak and Nesterov momentum methods and obtain the estimates of the regression coefficients, using both a constant step size and backtracking line search**

Polyak momentum with constant step size:

```{r polyak-constant}
# polyak momentum constant step
polyak_constant_step <- function(X, y, eta = NULL, tol = 1e-6, max_iter = 10000, xi = 0.5) {
  # Initialize
  n <- nrow(X)
  p <- ncol(X)
  beta <- rep(0, p)
  obj_values <- numeric(max_iter)
  eta_values <- numeric(max_iter)  # To store eta values used each iteration
  beta_values <- list() # To store beta values used each iteration
  eta_bt <- 1  # Initial step size for backtracking
  backtracking <- FALSE
  
  # Objective function: Mean Squared Error (MSE)
  obj_function <- function(beta) {
    sum((X %*% beta - y)^2) / (2 * n)
  }
  
  # Gradient function
  gradient <- function(beta) {
    t(X) %*% (X %*% beta - y) / n
  }


  
  for (iter in 1:max_iter) {
    grad <- gradient(beta)
    beta_values[[iter]] <- beta
    
    if (backtracking) {
      if (iter == 1) eta_bt <- 1  # Reset only in the first iteration
      beta_new <- beta - eta_bt * grad
      
      while (obj_function(beta_new) > obj_function(beta) - epsilon * eta_bt * sum(grad^2)) {
        eta_bt <- tau * eta_bt
        beta_new <- beta - eta_bt * grad
      }
      eta_used <- eta_bt
    } else {
      if (is.null(eta)) stop("When backtracking is FALSE, a fixed eta must be provided.")
      if(iter == 1) {
        y_k <- beta
      } else {
        beta_prev <- beta_values[[iter - 1]]
        y_k <- beta + xi * (beta - beta_prev)
      }
      beta_new <- y_k - eta * grad
      beta_values[[iter+1]] <- beta_new
      eta_used <- eta
    }
    
    eta_values[iter] <- eta_used
    
    obj_values[iter] <- obj_function(beta_new)
    
    if (sqrt(sum((beta_new - beta)^2)) < tol) {
      obj_values <- obj_values[1:iter]
      eta_values <- eta_values[1:iter]
      break
    }
    
    beta <- beta_new
  }
  
  return(list(beta = beta, obj_values = obj_values, eta_values = eta_values, beta_values = beta_values))
}

polyak_reg_constant_1 <- polyak_constant_step(X_dataset_1, y_dataset_1, eta = 5, tol = 1e-6, max_iter = 10000, xi = 0.5)

cat("Polyak Constant Step Size: dataset1 \n")
print("Beta Values:")
print(polyak_reg_constant_1$beta)
print("Obj Function Values:")
print(polyak_reg_constant_1$obj_values)
print("Eta Values:")
print(polyak_reg_constant_1$eta_values)
cat("The functions stopped after", max(which(!is.na(polyak_reg_constant_1$eta_values))), "iterations \n \n")

polyak_reg_constant_2 <- polyak_constant_step(X_dataset_2, y_dataset_2, eta = 0.02, tol = 1e-6, max_iter = 10000, xi = 0.5)

cat("Polyak Constant Step Size: dataset2 \n")
print("Beta Values:")
print(polyak_reg_constant_2$beta)
print("Obj Function Values:")
print(polyak_reg_constant_2$obj_values)
print("Eta Values:")
print(polyak_reg_constant_2$eta_values)
cat("The functions stopped after", max(which(!is.na(polyak_reg_constant_2$eta_values))), "iterations \n \n")
```

Nesterov Constant Step:

```{r nesterov-constant}
# nesterov momentum constant step
nesterov_constant_step <- function(X, y, eta = NULL, tol = 1e-6, max_iter = 10000) {
  # Initialize
  n <- nrow(X)
  p <- ncol(X)
  beta <- rep(0, p)
  obj_values <- numeric(max_iter)
  eta_values <- numeric(max_iter)  # To store eta values used each iteration
  beta_values <- list() # To store beta values used each iteration
  eta_bt <- 1  # Initial step size for backtracking
  backtracking <- FALSE
  
  # Objective function: Mean Squared Error (MSE)
  obj_function <- function(beta) {
    sum((X %*% beta - y)^2) / (2 * n)
  }
  
  # Gradient function
  gradient <- function(beta) {
    t(X) %*% (X %*% beta - y) / n
  }


  
  for (iter in 1:max_iter) {
    grad <- gradient(beta)
    beta_values[[iter]] <- beta
    
    if (backtracking) {
      if (iter == 1) eta_bt <- 1  # Reset only in the first iteration
      beta_new <- beta - eta_bt * grad
      
      while (obj_function(beta_new) > obj_function(beta) - epsilon * eta_bt * sum(grad^2)) {
        eta_bt <- tau * eta_bt
        beta_new <- beta - eta_bt * grad
      }
      eta_used <- eta_bt
    } else {
      if (is.null(eta)) stop("When backtracking is FALSE, a fixed eta must be provided.")
      if(iter == 1) {
        y_k <- beta
      } else {
        beta_prev <- beta_values[[iter - 1]]
        xi <- (iter - 1) / (iter + 2)
        y_k <- beta + xi * (beta - beta_prev)
      }
      beta_new <- y_k - eta * grad
      beta_values[[iter+1]] <- beta_new
      eta_used <- eta
    }
    
    eta_values[iter] <- eta_used
    
    obj_values[iter] <- obj_function(beta_new)
    
    if (sqrt(sum((beta_new - beta)^2)) < tol) {
      obj_values <- obj_values[1:iter]
      eta_values <- eta_values[1:iter]
      break
    }
    
    beta <- beta_new
  }
  
  return(list(beta = beta, obj_values = obj_values, eta_values = eta_values, beta_values = beta_values))
}

nesterov_reg_constant_1 <- nesterov_constant_step(X_dataset_1, y_dataset_1, eta = 5, tol = 1e-6, max_iter = 100000)

cat("Nesterov Constant Step Size: dataset1 \n")
print("Beta Values:")
print(nesterov_reg_constant_1$beta)
print("Obj Function Values:")
print(nesterov_reg_constant_1$obj_values)
print("Eta Values:")
print(nesterov_reg_constant_1$eta_values)
cat("The functions stopped after", max(which(!is.na(nesterov_reg_constant_1$eta_values))), "iterations \n \n")

nesterov_reg_constant_2 <- nesterov_constant_step(X_dataset_2, y_dataset_2, eta = 0.01, tol = 1e-6, max_iter = 100000)

cat("Nesterov Constant Step Size: dataset2 \n")
print("Beta Values:")
print(nesterov_reg_constant_2$beta)
print("Obj Function Values:")
print(nesterov_reg_constant_2$obj_values)
print("Eta Values:")
print(nesterov_reg_constant_2$eta_values)
cat("The functions stopped after", max(which(!is.na(nesterov_reg_constant_2$eta_values))), "iterations \n \n")
```

BLS with Polyak:

```{r polyak-bls}
# polyak momentum constant step
polyak_bls <- function(X, y, eta = NULL, tol = 1e-6, max_iter = 10000, xi = 0.5, epsilon = 0.5, tau = 0.5, backtracking=TRUE) {
  # Initialize
  n <- nrow(X)
  p <- ncol(X)
  beta <- rep(0, p)
  obj_values <- numeric(max_iter)
  eta_values <- numeric(max_iter)  # To store eta values used each iteration
  beta_values <- list() # To store beta values used each iteration
  eta_bt <- 1  # Initial step size for backtracking
  
  # Objective function: Mean Squared Error (MSE)
  obj_function <- function(beta) {
    sum((X %*% beta - y)^2) / (2 * n)
  }
  
  # Gradient function
  gradient <- function(beta) {
    t(X) %*% (X %*% beta - y) / n
  }


  
  for (iter in 1:max_iter) {
    grad <- gradient(beta)
    beta_values[[iter]] <- beta
    
    if (backtracking) {
      if (iter == 1) {
        eta_bt <- 1 # Reset only in the first iteration
        y_k <- beta
      }
      else {
        beta_prev <- beta_values[[iter - 1]]
        y_k <- beta + xi * (beta - beta_prev)
      }
      beta_new <- y_k - eta_bt * grad
      
      while (obj_function(beta_new) > obj_function(beta) - epsilon * eta_bt * sum(grad^2)) {
        eta_bt <- tau * eta_bt
        beta_new <- beta - eta_bt * grad
      }
      eta_used <- eta_bt
    } else {
      if (is.null(eta)) stop("When backtracking is FALSE, a fixed eta must be provided.")
      if(iter == 1) {
        y_k <- beta
      } else {
        beta_prev <- beta_values[[iter - 1]]
        y_k <- beta + xi * (beta - beta_prev)
      }
      beta_new <- y_k - eta * grad
      beta_values[[iter+1]] <- beta_new
      eta_used <- eta
    }
    
    eta_values[iter] <- eta_used
    
    obj_values[iter] <- obj_function(beta_new)
    
    if (sqrt(sum((beta_new - beta)^2)) < tol) {
      obj_values <- obj_values[1:iter]
      eta_values <- eta_values[1:iter]
      break
    }
    
    beta <- beta_new
  }
  
  return(list(beta = beta, obj_values = obj_values, eta_values = eta_values, beta_values = beta_values))
}

polyak_bls_1 <- polyak_bls(X_dataset_1, y_dataset_1, eta = NULL, tol = 1e-6, max_iter = 10000, xi = 0.5, backtracking=TRUE, epsilon = 0.5, tau = 0.5)

cat("Polyak BLS Step Size: dataset1 \n")
print("Beta Values:")
print(polyak_bls_1$beta)
print("Obj Function Values:")
print(polyak_bls_1$obj_values)
print("Eta Values:")
print(polyak_bls_1$eta_values)
cat("The functions stopped after", max(which(!is.na(polyak_bls_1 $eta_values))), "iterations \n \n")

polyak_bls_2 <- polyak_bls(X_dataset_2, y_dataset_2, eta = NULL, tol = 1e-6, max_iter = 10000, xi = 0.5, backtracking=TRUE, epsilon = 0.5, tau = 0.5)

cat("Polyak BLS Step Size: dataset2 \n")
print("Beta Values:")
print(polyak_bls_2$beta)
print("Obj Function Values:")
print(polyak_bls_2$obj_values)
print("Eta Values:")
print(polyak_bls_2$eta_values)
cat("The functions stopped after", max(which(!is.na(polyak_bls_2$eta_values))), "iterations \n \n")
```


BLS with Nesterov:

```{r nesterov-bls}
# nesterov momentum constant step
nesterov_bls <- function(X, y, eta = NULL, tol = 1e-6, max_iter = 10000, epsilon = 0.5, tau = 0.5, backtracking=TRUE) {
  # Initialize
  n <- nrow(X)
  p <- ncol(X)
  beta <- rep(0, p)
  obj_values <- numeric(max_iter)
  eta_values <- numeric(max_iter)  # To store eta values used each iteration
  beta_values <- list() # To store beta values used each iteration
  eta_bt <- 1  # Initial step size for backtracking
  
  # Objective function: Mean Squared Error (MSE)
  obj_function <- function(beta) {
    sum((X %*% beta - y)^2) / (2 * n)
  }
  
  # Gradient function
  gradient <- function(beta) {
    t(X) %*% (X %*% beta - y) / n
  }


  
  for (iter in 1:max_iter) {
    grad <- gradient(beta)
    beta_values[[iter]] <- beta
    
    if (backtracking) {
      if (iter == 1) {
        eta_bt <- 1 # Reset only in the first iteration
        y_k <- beta
      }
      else {
        beta_prev <- beta_values[[iter - 1]]
        xi <- (iter - 1) / (iter + 2)
        y_k <- beta + xi * (beta - beta_prev)
      }
      beta_new <- y_k - eta_bt * grad
      
      while (obj_function(beta_new) > obj_function(beta) - epsilon * eta_bt * sum(grad^2)) {
        eta_bt <- tau * eta_bt
        beta_new <- beta - eta_bt * grad
      }
      eta_used <- eta_bt
    } else {
      if (is.null(eta)) stop("When backtracking is FALSE, a fixed eta must be provided.")
      if(iter == 1) {
        y_k <- beta
      } else {
        beta_prev <- beta_values[[iter - 1]]
        y_k <- beta + xi * (beta - beta_prev)
      }
      beta_new <- y_k - eta * grad
      beta_values[[iter+1]] <- beta_new
      eta_used <- eta
    }
    
    eta_values[iter] <- eta_used
    
    obj_values[iter] <- obj_function(beta_new)
    
    if (sqrt(sum((beta_new - beta)^2)) < tol) {
      obj_values <- obj_values[1:iter]
      eta_values <- eta_values[1:iter]
      break
    }
    
    beta <- beta_new
  }
  
  return(list(beta = beta, obj_values = obj_values, eta_values = eta_values, beta_values = beta_values))
}

nesterov_bls_1 <- nesterov_bls(X_dataset_1, y_dataset_1, eta = NULL, tol = 1e-6, max_iter = 10000, backtracking=TRUE, epsilon = 0.5, tau = 0.5)

cat("Nesterov BLS Step Size: dataset1 \n")
print("Beta Values:")
print(nesterov_bls_1$beta)
print("Obj Function Values:")
print(nesterov_bls_1$obj_values)
print("Eta Values:")
print(nesterov_bls_1$eta_values)
cat("The functions stopped after", max(which(!is.na(nesterov_bls_1 $eta_values))), "iterations \n \n")

nesterov_bls_2 <- nesterov_bls(X_dataset_2, y_dataset_2, eta = NULL, tol = 1e-6, max_iter = 10000, backtracking=TRUE, epsilon = 0.5, tau = 0.5)

cat("Nesterov BLS Step Size: dataset2 \n")
print("Beta Values:")
print(nesterov_bls_2$beta)
print("Obj Function Values:")
print(nesterov_bls_2$obj_values)
print("Eta Values:")
print(nesterov_bls_2$eta_values)
cat("The functions stopped after", max(which(!is.na(nesterov_bls_2$eta_values))), "iterations \n \n")
```

### 1) Compare again the estimates obtained from the two momentum methods with the least-squares solution by calculating ∥ˆβGD − ˆβ∥2

Calculate the difference of Betas:

```{r}
# transform dataset 1 coefs
beta_gd_d1_p_const <- as.vector(polyak_reg_constant_1$beta)
p1 <- data.frame(Coef_data1_polyak_constant = names, Beta_GD = beta_gd_d1_p_const, Beta = lm_1)

beta_gd_d1_n_const <- as.vector(nesterov_reg_constant_1$beta)
n1 <- data.frame(Coef_data1_nesterov_constant = names, Beta_GD = beta_gd_d1_n_const, Beta = lm_1)

# transform dataset 2 coefs
beta_gd_d2_p_const <- as.vector(polyak_reg_constant_2$beta)
p2 <- data.frame(Coef_data2_polyak_constant = names, Beta_GD = beta_gd_d2_p_const, Beta = lm_2)

beta_gd_d2_n_const <- as.vector(nesterov_reg_constant_2$beta)
n2 <- data.frame(Coef_data1_nesterov_constant = names, Beta_GD = beta_gd_d2_n_const, Beta = lm_2)

# show the difference between Beta_GD and Beta
p1 <- p1 %>%
  mutate(`Beta_GD - Beta` = Beta_GD - Beta)

p2 <- p2 %>%
  mutate(`Beta_GD - Beta` = Beta_GD - Beta)

n1 <- n1 %>%
  mutate(`Beta_GD - Beta` = Beta_GD - Beta)

n2 <- n2 %>%
  mutate(`Beta_GD - Beta` = Beta_GD - Beta)


print(p1)
print(p2)

print(n1)
print(n2)

```

Calculating the norms:

```{r}
p1_norm <- sqrt(sum(p1$`Beta_GD - Beta`^2))
p2_norm <- sqrt(sum(p2$`Beta_GD - Beta`^2))
n1_norm <- sqrt(sum(n1$`Beta_GD - Beta`^2))
n2_norm <- sqrt(sum(n2$`Beta_GD - Beta`^2))
cat("Dataset 1 Normm Polyak:", p1_norm, "\n")
cat("Dataset 2 Norm Polyak:", p2_norm, "\n")
cat("Dataset 1 Norm Nesterov:", n1_norm, "\n")
cat("Dataset 2 Norm Nesterov:", n2_norm, "\n")
```


### 2) Plot the value of the objective function as a function of the number of iterations required, and comment whether the momentum methods reduce the number of iterations requires to obtain the regression coeffcients (using the same tolerance)

```{r plots-2-b-2}
#| fig-width: 10
plot(polyak_reg_constant_1$obj_values, type = "o", col = "blue", pch = 16, cex = 0.5,
     xlab = "Iteration", ylab = "Objective Function Value",
     main = "Polyak Const Step, dataset1")
plot(polyak_reg_constant_2$obj_values, type = "o", col = "blue", pch = 16, cex = 0.5,
     xlab = "Iteration", ylab = "Objective Function Value",
     main = "Polyak Const Step, dataset2")
plot(polyak_bls_1$obj_values, type = "o", col = "blue", pch = 16, cex = 0.5,
     xlab = "Iteration", ylab = "Objective Function Value",
     main = "Polyak BLS, dataset1")
plot(polyak_bls_2$obj_values, type = "o", col = "blue", pch = 16, cex = 0.5,
     xlab = "Iteration", ylab = "Objective Function Value",
     main = "Polyak BLS, dataset2")
plot(nesterov_reg_constant_1$obj_values, type = "o", col = "blue", pch = 16, cex = 0.5,
     xlab = "Iteration", ylab = "Objective Function Value",
     main = "Nesterov Const Step, dataset1")
plot(nesterov_reg_constant_2$obj_values, type = "o", col = "blue", pch = 16, cex = 0.5,
     xlab = "Iteration", ylab = "Objective Function Value",
     main = "Nesterov Const Step, dataset2")
plot(nesterov_bls_1$obj_values, type = "o", col = "blue", pch = 16, cex = 0.5,
     xlab = "Iteration", ylab = "Objective Function Value",
     main = "Nesterov BLS, dataset1")
plot(nesterov_bls_2$obj_values, type = "o", col = "blue", pch = 16, cex = 0.5,
     xlab = "Iteration", ylab = "Objective Function Value",
     main = "Nesterov BLS, dataset2")
```

We can see for Polyak and Nesterov, both of the BLS methods did not necessarily converge slower. This was likely because we were able to manually select a more agressive step size that what is calculated with BLS, or in other case BLS chose a better step size with the momentum shift. We see that Nesterov took way more iterations with constant step than polyak constant step. We see that Nesterov BLS is much better than Nesterov at a constant step size. Finally, we see that Polyak BLS took more iterations, likely because the step size was safer tha with constant step.

### 3) Comment on the results; namely, the difference in the accuracy of the solution and the standard gradient descent algorithm
a
Looking at the accuacy of the solution, we see that the accuracy norms for the standard gradient descent is 0.195615 and 0.06. For polyak we got .195615 and 0.05539779, and for Nesterov we got .1956129 and 0.05456602. This might indicate that the norms are very close together, and since we used the same tolerance we expect all gradient descent algorithms to be within that solerance. Therefore the algorithms converged to the same answer.