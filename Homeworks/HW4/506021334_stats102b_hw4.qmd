---
title: "Stats 102B HW 4"
author: "Bryan Mui - UID 506021334"
execute:
  cache: true
format:
  pdf:
    geometry: left=0.3in, right=0.3in, top=0.3in, bottom=0.3in
    keep-tex: true
    include-in-header:
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
         \DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\{\}}
---

Due Wed, June 4, 11:00 pm

```{r read-data}
set.seed(777)
library(tidyverse)
library(xtable)
train <- read_csv("train_data.csv")
val <- read_csv("validation_data.csv")
```

\newpage

## Problem 1

Consider the function

$$
f(x) = \frac{1}{4} x^4 - x^2 + 2x
$$

### Part $(\alpha)$

Using the pure version of Newton’s algorithm report $x_k$ for $k = 20$ (after running the algorithm for 20 iterations) based on the following 5 initial points:

1.  $x_0 = −1$
2.  $x_0 = 0$
3.  $x_0 = 0.1$
4.  $x_0 = 1$
5.  $x_0 = 2$

**Newton's pure algorithm is as follows:**

1.  Select $x_0 \in \mathbb{R}^n$
2.  While stopping criterion \> tolerance do:
    1.  $x_{k+1} = x_k - [\nabla^2f(x_k)]^{-1} \nabla f(x_k)$
    2.  Calculate value of stopping criterion($|f(x_{k+1}) - f(x_k)| \leq \epsilon$)

Gradient: $\nabla f(x) = \frac{\partial}{\partial x} = f'(x) = x^3 - 2x + 2$

Hessian: $\nabla^2 f(x) = \frac{\partial^2}{\partial x^2} = f''(x) = 3x^2 - 2$

```{r, 1-alpha}
# params
max_iter <- 20
starting_points <- c(-1, 0, 0.1, 1, 2)
stopping_tol <- 1e-6


# algorithm
newton_pure_alg <- function(max_iter, starting_point, stopping_tol) {
  beta <- starting_point
  iterations_ran <- 0
  betas_vec <- c(beta)
  
  obj <- function(x) {
  return(1/4 * x^4 - x^2 + 2*x)
  }
  grad <- function(x) {
    x^3 - 2*x + 2
  }
  hessian <- function(x) {
    3*x^2 - 2
  }
  
  # Training loop
  for(i in 1:max_iter) {
    beta_new <- beta - (grad(beta) / hessian(beta))
    betas_vec[i+1] <- beta_new
    if(abs(beta_new - beta) <= stopping_tol) { break }
    beta <- beta_new
  }
  iterations_ran <- i
  return(list(iterations=iterations_ran, betas=betas_vec))
}

# running the alg
cat("Newton's Algorithm(Pure) For Different Starting Points: \n")
for (starting_point in starting_points) {
  result <- newton_pure_alg(max_iter, starting_point, stopping_tol)
  cat("\nStarting Point:", starting_point, "\nIterations:", result$iterations, "\nBetas:\n")
  print(result$betas)
  cat("\n", "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~", "\n")
}
```

### Part (i) What do you observe?

![Plot of the Objective Function f(x)](images/clipboard-3935267595.png)

I observe that given the code output, the algorithms that did not converge oscillate between 0 1 0 1, which means that it gets stuck when the curvature of the graph changes. I think that the graph goes from concave up to concave down at this point, which will mess up the gradient descent calculation. The Hessian is the second derivative, or second order gradient, meaning the concavity of the function will affect the sign. Given the update step, $x_{k+1} = x_k - [\nabla^2f(x_k)]^{-1} \nabla f(x_k)$, we can see that if the Hessian's sign goes from negative to positive(concave down to concave up) or vice versa then it will change the sign of the update step to be gradient ascent(away from the minimum) rather than towards it

### Part (ii) How can you fix the issue reported in (i)?

I believe that the issue is normally fixed by drawing the graph and getting and understanding of the shape, and therefore choosing one of the starting points that does end up converging. However, in cases where that isn't possible(if the function cannot be plotted or is very complex), I would try to build a vector of possible starting points and perform a grid search where we run the algorithm on multiple starting points and various parameters. This would allow us to find a starting point where the algorithm would converge. Although the grid search is an easy fix, switching algorithms is a better fix. From the lecture slides, there are alternative Newton's algorithms like the Levenberg-Marquard algorithm, which is a fix for when the Hessian is not positive definite by essentially translating the Hessian to positive definite through transforming the eigenvalues of the matrix, which I would implement if there is too much trouble with the pure Newton's algorithm.

\newpage

## Problem 2

Consider the data in the train data.csv file. The first 600 columns correspond to the predictors and the last column to the response y.

### Part (i) Implement that proximal gradient algorithm for Lasso regression, by modifying appropriately your code from Homework 1.

Proximal Gradient Descent is good for problems in the form:

$$
\underset{x}{\min} F(x) = f(x) + g(x), \space x \in \mathbb{R}^n
$$

Where f and g are functions with a global minimum, f is differentiable, and g is not differentiable. In this case, we have $MSE=f(x)=\frac{1}{n}||y-X\beta||_2^2$ and $g(x)=\lambda||\beta||_1$

**The Proximal Gradient Descent Algorithm(General):**

1.  Select $x_0 \in \mathbb{R}^n$
2.  While stopping criterion \> tolerance do:
    1.  $y_k = x_k + \eta_k \nabla{f(x_k)}$
    2.  $x_{k+1} = \text{prox}_{\eta_k,g}(y_k)$
    3.  Calculate value of stopping criterion($|f(x_{k+1}) - f(x_k)| \leq \epsilon$)

Proximal Operator: $\text{prox}_{\eta_k,g}(y_k) = \underset{x}{\text{argmin}}\{ g(x) + \frac{1}{2\eta_k}||x-y_k||^2_2 \}$, where,

-   $\eta_k$ is the step size at the previous iteration,

-   $y_k$ is the gradient step at the previous iteration,

-   $\text{argmin}$ is the minimum value of the function

The Proximal Function for L1-Regularization(LASSO), as shown in class:

$$
\text{prox}_{t,\lambda||\cdot||_1}(z) = S_{t,\lambda}(z) = \text{sign}(z) \cdot \max(|z|-t\lambda,\space0)
$$

```{r 2-1}
# Params
X_train <- train %>%
  select(-y) %>%
  as.matrix()
y_train <- train %>%
  select(y) %>%
  as.matrix()
X_val <- val %>%
  select(-y) %>%
  as.matrix()
y_val <- val %>%
  select(y) %>%
  as.matrix()


# Implement Proximal Gradient Descent
proximal_gradient_descent_lasso <- function(X, y, eta, lambda, tol, max_iter) {
  n <- nrow(X)
  p <- ncol(X)
  beta <- rep(0, p)
  obj_values <- c() # obj values result
  eta_values <- c()  # eta values result
  sse_train_loss <- c()  # SSE train loss result
  sse_test_loss <- c()  # SSE test loss result
  beta_values <- matrix(0, nrow = max_iter, ncol = p) # betas result
  
  
  # Objective function: Mean Squared Error (MSE)
  obj_function <- function(beta) {
    sum((X %*% beta - y)^2) / (2 * n)
  }
  
  # Gradient function
  gradient <- function(beta) {
    t(X) %*% (X %*% beta - y) / n
  }
  
  # Proximal Function
  prox <- function(eta, lambda, y_k) {
    sign(y_k) * pmax(abs(y_k) - eta * lambda, 0)
  }
  
  # Grad Descent Step
  for (iter in 1:max_iter) {
    grad <- gradient(beta)
    
    y_k <- beta - eta * grad
    beta_new <- prox(eta, lambda, y_k)
    
    # Storing the values for result
    eta_values[iter] <- eta
    beta_values[iter, ] <- beta_new
    obj_values[iter] <- obj_function(beta_new)
    # Storing the SSE losses
    # get prediction and calculate SSE-train-loss
    # y = XB
    y_pred <- X_train %*% beta_new
    sse_train_loss[iter] <- sum((y_train - y_pred)^2)
    
    # get prediction and calculate SSE-val-loss
    # y = XB
    y_pred <- X_val %*% beta_new
    sse_val_loss[iter] <- sum((y_val - y_pred)^2)
    
    # Stop crit
    if (norm(beta_new - beta, type = "2") < tol) {break}
    
    beta <- beta_new
  }
  
  return(list(iterations=iter, beta_fin=beta, beta_values=beta_values, obj_values=obj_values, eta_values=eta_values, eta=eta, lambda=lambda, sse_train_loss=sse_train_loss, sse_val_loss=sse_val_loss))
}
```

```{r}
m1 <- proximal_gradient_descent_lasso(X_train, y_train, 0.01, 0.1, 1e-6, 10000)
```

```{r}
cat("Iterations:", m1$iterations, "\n")
cat("Obj Values(Last 10):", "\n")
tail(m1$obj_values, n = 10)
cat("Final Beta(first 10):", "\n")
print(m1$beta_fin[1:10])
cat("SSE Train Loss(last 10):", "\n")
tail(m1$sse_train_loss, n=10)
cat("SSE Val Loss(last 10):", "\n")
tail(m1$sse_val_loss, n=10)
```

### Part (ii) To select a good value for the regularization parameter $λ$ use the data in the validation data.csv to calculate the sum-of-squares error validation loss.

```{r 2-3-grid-search}
# Grid Search Params
tol <- 1e-6
max_iter <- 10000
eta <- 0.01
# lambdas <- c(
#   0.001,
#   0.01, 0.02, 0.05,
#   0.1, 0.2, 0.5,
#   1, 2, 5, 10, 20, 50
# )
lambdas <- c(0.001, 0.01, 0.1)

# Initialize storing structures
models_df <- data.frame(
  Model = integer(),
  Lambda = numeric(),
  Iterations = integer(),
  Converged = logical(),
  SSE_Train_Loss = numeric(),
  SSE_Val_Loss = numeric()
)
models <- list()

# Running the Code
for (i in 1:length(lambdas)) {
    # train the model on specific lambda
    lambda <- lambdas[i]
    result <- proximal_gradient_descent_lasso(X_train, y_train, eta, lambda, tol, max_iter)
    
    # get prediction and calculate SSE-train-loss
    # y = XB
    y_pred <- X_train %*% result$beta_fin
    sse_train_loss <- sum((y_train - y_pred)^2)
    
    # get prediction and calculate SSE-val-loss
    # y = XB
    y_pred <- X_val %*% result$beta_fin
    sse_val_loss <- sum((y_val - y_pred)^2)
    
    # Output the model
    cat("\nModel:", i, "| Lambda:", result$lambda, "| Iterations:", result$iterations, "| Converged?:", result$iterations < max_iter, "| SSE Train Loss:", sse_train_loss, "| SSE Val Loss:", sse_val_loss, "\n")
    
    # Store row in the results data frame
    models_df <- rbind(models_df, data.frame(
      Model = i,
      Lambda = result$lambda,
      Iterations = result$iterations,
      Converged = result$iterations < max_iter,
      SSE_Train_Loss = sse_train_loss,
      SSE_Val_Loss = sse_val_loss
    ))
    
    # Store the model params in a list for later retrieval
    models[[i]] <- result
}
```

```{r}
# models_df
latex_table <- xtable(models_df, digits = 4, caption = "Model Results For Different Lambdas")
print(latex_table, include.rownames = FALSE)
```


### Part(iii) Show a plot of the training and validation loss as a function of iterations. 

Getting the Best Candidates:

Plotting Training Loss:

```{r 2-3-train-loss}

```

Plotting Validation Loss:

```{r 2-3-validation-loss}

```

### Part(iv) Report the number of regression coefficients estimated as zero based on the best value of $λ$ you selected.

```{r}

```

